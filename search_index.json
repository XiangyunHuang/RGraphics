[
["index.html", "数据可视化与R语言 Data Visualization with R 欢迎 Why R 目标读者 获取帮助 发展历史 记号约定 运行环境", " 数据可视化与R语言 Data Visualization with R 黄湘云 2019-07-27 07:37:42 CST 欢迎 这本书还处于一个很早期的阶段 Why R GNU R 是发布在 GPL-2/3 下的开源自由软件，意味着只要你遵循该协议，就可以自由地获取、修改和发布R 源代码，R 本身的这种开源自由的属性，决定你可以免费地使用它。《The Art of R Programming》的作者 Norm Matloff 给出使用 R 语言的四个优势：它是统计学家开发的，也是为统计学家打造的；内建的矩阵类型和矩阵操作非常高效；不管是来自基础 R 还是 CRAN 上的绘图包，都提供强大的绘图功能；还有优秀的并行能力1，最近他更是在数据科学中全面比较了 R 与 Python2。关于 R 语言和 Python 语言的对比，网络上充斥着很多的文章，除了赞扬，还有表示反对的声音，如 R语言采用的对 GPL 协议3，甚至有人列举了逃离 R 语言阵营的10大上榜理由4，datacamp 提供了一份较为完整的对比图，仅供参考5。如果你是学统计的学生或者数据分析师，我都建议你先学习 R6，如果你是社会科学的学生和研究者， R 社区开发了 GUI 工具，如 Rcmdr 和 rattle，还有基于 Shiny 的分析工具 radiant 和类似 SPSS 的 JASP。 R 语言比较遭人诟病的大概有： R 包总体数量已达到 15000+，年度增长速度大约在 4.6% 左右，很多 R 包都在重复造轮子，且 R 包之间依赖关系非常复杂。若与 Python 作一个对比，所有的 R 包和 Python 模块必须处于活跃维护，拥有大批粉丝，维护者在社区内享有声誉，有厂子或科研经费支持。我们不打嘴仗，不下结论，只做对比，不完善之处还请大家指出并补充，见表 0.1。 每个 Base R 包内的函数非常多，参数也非常多，功能涉及方方面面，初学者学习起来难度非常大！数据处理和可视化常用基本包最流行的 tidyverse 系列和基础 R 系统存在很多不一致，在不清楚的情况下很难掌握，而陷于已有的函数不能自拔！ R 是面向对象的程序设计语言，是解释性的语言，也是函数式编程语言，包含的程序设计风格非常多，仅面向对象的设计就有 S3、 S4、 RC、 R5 和 R6。每一个操作都是函数调用，一切皆是对象的环境和闭包概念简洁又复杂。 R 内置的数据结构非常多，原子类型的有字符、布尔、整型、复数、双精度浮点、单精度浮点等，此外常见的还有数据框、列表。每个特定的领域往往还有特殊的类型，如时间序列 ts、zoo 等， 空间对象 sp、 raster 和 sf 等。 深入学习 R 实现的统计模型，如 lm、glm 等，你可能会发现统计学家的程序设计思维如此难懂。 Thomas Lumley, “R Fundamentals and Programming Techniques” https://faculty.washington.edu/tlumley/Rcourse/R-fundamentals.pdf 表 0.1: R 与 Python 常用模块对照表 比较内容 具体范围 R 包 Python 模块 数据获取 本地、数据库、远程 内置，RCurl、XML、rvest、data.table、 odbc scrapy 数据清理 正则表达式 内置，stringi、stringr、tidyr re 数据聚合 SQL支持的所有操作 内置，dplyr、purrr、dbplyr、sparklyr Numpy、Scipy、Pandas 数据分析 统计推断的所有方法 内置，lme4、rstan、mxnet、xgoost、 tensorflow xgboost、scikit-learn、tensorflow、mxnet 数据展示 数据可视化 内置，ggplot2、plotly matplotlib、bokeh、plotly 数据报告 网页文档、幻灯片 rmarkdown、bookdown、blogdown 数据落地 模型部署，调优，维护 plumber、opencpu、fiery 目标读者 本书起源于自己的学习笔记，侧重统计图形，当然也包括在制作统计图形之前的数据导入和ETL操作，后续的数据可视化。本书的目标可以是接触过 R 语言的读者，也可以是零基础者，书的内容侧重数据处理和可视化分析，数据建模的部分比较少。 获取帮助 R 语言官网给出了一份如何获取帮助的指导 https://www.r-project.org/help.html，RStudio 公司也总结了一份 Getting Help with R，又及 https://blog.rsquaredacademy.com/getting-help-in-r-updated/ 发展历史 GNU R 最初由 Ross Ihaka 和 Robert Gentleman 开发，它脱胎于 S 语言，S 语言形成于大名鼎鼎的美国贝尔实验室，距今已有40多年的历史了7，R语言的前世今生8，Ross Ihaka 总结了过去的经验，展望了 R 语言未来发展的方向9。 记号约定 写作风格，R 包名称都加粗表示，如 bookdown， rmarkdown 等，软件、编程语言名称保持原样，如 TinyTeX，LyX，TeXLive，R，Python，Stan，C++，SQL等，在代码块中，我们不使用R&gt;或+，代码输出结果用#&gt;注释。knitr (Xie 2015)、 bookdown (Xie 2016)、 Pandoc 和 TinyTeX ，请使用 XeLaTeX 编译这本书，等宽字体为 inconsolata 默认的文本字体为 Times 运行环境 重现书籍本节内容需要的 R 包列表如下 xfun::session_info(c(&quot;rmarkdown&quot;, &quot;bookdown&quot;)) #&gt; R version 3.6.1 (2019-07-05) #&gt; Platform: x86_64-pc-linux-gnu (64-bit) #&gt; Running under: Debian GNU/Linux 10 (buster) #&gt; #&gt; Locale: #&gt; LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C #&gt; LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 #&gt; LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 #&gt; LC_PAPER=en_US.UTF-8 LC_NAME=C #&gt; LC_ADDRESS=C LC_TELEPHONE=C #&gt; LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C #&gt; #&gt; Package version: #&gt; base64enc_0.1.3 bookdown_0.12 digest_0.6.20 evaluate_0.14 #&gt; glue_1.3.1 graphics_3.6.1 grDevices_3.6.1 highr_0.8 #&gt; htmltools_0.3.6 jsonlite_1.6 knitr_1.23 magrittr_1.5 #&gt; markdown_1.0 methods_3.6.1 mime_0.7 Rcpp_1.0.2 #&gt; rmarkdown_1.14 stats_3.6.1 stringi_1.4.3 stringr_1.4.0 #&gt; tinytex_0.14 tools_3.6.1 utils_3.6.1 xfun_0.8 #&gt; yaml_2.2.0 #&gt; #&gt; Pandoc version: 2.7.3 本书要求 R 软件版本 3.6.1 因为书中涉及 barplot 新增的公式方法，新增多维数组操作函数 asplit， axis 函数的 gap.axis ，新增 hcl.colors 函数等，完整列表见官网 What’s New? 书籍同时使用 bookdown.org 和 netlify 部署，网址分别是 https://bookdown.org/xiangyun/RGraphics/ 和 https://r-graphics.netlify.com/ 参考文献 "],
["preface.html", "前言 关于本书 本书结构 关于作者", " 前言 关于本书 这里写每章的主要内容介绍 本书结构 关于作者 热心开源事业，统计之都副主编，经常混迹于统计之都论坛、Github 和爆栈网。个人主页 https://www.xiangyunhuang.com.cn/ "],
["dm-import-export.html", "第 1 章 数据搬运工 1.1 导入数据 1.2 其它数据格式 1.3 导入大数据集 1.4 从数据库导入 1.5 批量导入数据 1.6 批量导出数据 1.7 导出数据 1.8 运行环境", " 第 1 章 数据搬运工 导入数据与导出数据，各种数据格式，数据库 1.1 导入数据 Base R 针对不同的数据格式文件，提供了大量的数据导入和导出函数，不愧是专注数据分析20余年的优秀统计软件。 除了函数 write.ftable 和 read.ftable 来自 stats 包，都来自 base 和 utils 包 # 当前环境的搜索路径 searchpaths() #&gt; [1] &quot;.GlobalEnv&quot; &quot;/usr/lib/R/library/stats&quot; #&gt; [3] &quot;/usr/lib/R/library/graphics&quot; &quot;/usr/lib/R/library/grDevices&quot; #&gt; [5] &quot;/usr/lib/R/library/utils&quot; &quot;/usr/lib/R/library/datasets&quot; #&gt; [7] &quot;/usr/lib/R/library/methods&quot; &quot;Autoloads&quot; #&gt; [9] &quot;/usr/lib/R/library/base&quot; # 返回匹配结果及其所在路径的编号 apropos(&quot;^(read|write)&quot;, where = TRUE, mode = &quot;function&quot;) #&gt; 5 5 9 #&gt; &quot;read.csv&quot; &quot;read.csv2&quot; &quot;read.dcf&quot; #&gt; 5 5 5 #&gt; &quot;read.delim&quot; &quot;read.delim2&quot; &quot;read.DIF&quot; #&gt; 5 2 5 #&gt; &quot;read.fortran&quot; &quot;read.ftable&quot; &quot;read.fwf&quot; #&gt; 5 5 9 #&gt; &quot;read.socket&quot; &quot;read.table&quot; &quot;readBin&quot; #&gt; 9 5 9 #&gt; &quot;readChar&quot; &quot;readCitationFile&quot; &quot;readline&quot; #&gt; 9 9 9 #&gt; &quot;readLines&quot; &quot;readRDS&quot; &quot;readRenviron&quot; #&gt; 9 5 5 #&gt; &quot;write&quot; &quot;write.csv&quot; &quot;write.csv2&quot; #&gt; 9 2 5 #&gt; &quot;write.dcf&quot; &quot;write.ftable&quot; &quot;write.socket&quot; #&gt; 5 9 9 #&gt; &quot;write.table&quot; &quot;writeBin&quot; &quot;writeChar&quot; #&gt; 9 #&gt; &quot;writeLines&quot; 1.1.1 scan scan(file = &quot;&quot;, what = double(), nmax = -1, n = -1, sep = &quot;&quot;, quote = if(identical(sep, &quot;\\n&quot;)) &quot;&quot; else &quot;&#39;\\&quot;&quot;, dec = &quot;.&quot;, skip = 0, nlines = 0, na.strings = &quot;NA&quot;, flush = FALSE, fill = FALSE, strip.white = FALSE, quiet = FALSE, blank.lines.skip = TRUE, multi.line = TRUE, comment.char = &quot;&quot;, allowEscapes = FALSE, fileEncoding = &quot;&quot;, encoding = &quot;unknown&quot;, text, skipNul = FALSE) 首先让我们用 cat 函数创建一个练习数据集 ex.data cat(&quot;TITLE extra line&quot;, &quot;2 3 5 7&quot;, &quot;11 13 17&quot;) #&gt; TITLE extra line 2 3 5 7 11 13 17 cat(&quot;TITLE extra line&quot;, &quot;2 3 5 7&quot;, &quot;11 13 17&quot;, file = &quot;data/ex.data&quot;, sep = &quot;\\n&quot;) 以此练习数据集，介绍 scan 函数最常用的参数 scan(&quot;data/ex.data&quot;) #&gt; Error in scan(&quot;data/ex.data&quot;): scan() expected &#39;a real&#39;, got &#39;TITLE&#39; 从上面的报错信息，我们发现 scan 函数只能读取同一类型的数据，如布尔型 logical， 整型 integer，数值型 numeric(double)， 复数型 complex，字符型 character，raw 和列表 list。所以我们设置参数 skip = 1 把第一行跳过，就成功读取了数据 scan(&quot;data/ex.data&quot;, skip = 1) #&gt; [1] 2 3 5 7 11 13 17 如果设置参数 quiet = TRUE 就不会报告读取的数据量 scan(&quot;data/ex.data&quot;, skip = 1, quiet = TRUE) #&gt; [1] 2 3 5 7 11 13 17 参数 nlines = 1 表示只读取一行数据 scan(&quot;data/ex.data&quot;, skip = 1, nlines = 1) # only 1 line after the skipped one #&gt; [1] 2 3 5 7 默认参数 flush = TRUE 表示读取最后一个请求的字段后，刷新到行尾，下面对比一下读取的结果 scan(&quot;data/ex.data&quot;, what = list(&quot;&quot;, &quot;&quot;, &quot;&quot;)) # flush is F -&gt; read &quot;7&quot; #&gt; Warning in scan(&quot;data/ex.data&quot;, what = list(&quot;&quot;, &quot;&quot;, &quot;&quot;)): number of items #&gt; read is not a multiple of the number of columns #&gt; [[1]] #&gt; [1] &quot;TITLE&quot; &quot;2&quot; &quot;7&quot; &quot;17&quot; #&gt; #&gt; [[2]] #&gt; [1] &quot;extra&quot; &quot;3&quot; &quot;11&quot; &quot;&quot; #&gt; #&gt; [[3]] #&gt; [1] &quot;line&quot; &quot;5&quot; &quot;13&quot; &quot;&quot; scan(&quot;data/ex.data&quot;, what = list(&quot;&quot;, &quot;&quot;, &quot;&quot;), flush = TRUE) #&gt; [[1]] #&gt; [1] &quot;TITLE&quot; &quot;2&quot; &quot;11&quot; #&gt; #&gt; [[2]] #&gt; [1] &quot;extra&quot; &quot;3&quot; &quot;13&quot; #&gt; #&gt; [[3]] #&gt; [1] &quot;line&quot; &quot;5&quot; &quot;17&quot; 临时文件 ex.data 用完了，我们调用 unlink 函数将其删除，以免留下垃圾文件 unlink(&quot;data/ex.data&quot;) # tidy up 1.1.2 read.table read.table(file, header = FALSE, sep = &quot;&quot;, quote = &quot;\\&quot;&#39;&quot;, dec = &quot;.&quot;, numerals = c(&quot;allow.loss&quot;, &quot;warn.loss&quot;, &quot;no.loss&quot;), row.names, col.names, as.is = !stringsAsFactors, na.strings = &quot;NA&quot;, colClasses = NA, nrows = -1, skip = 0, check.names = TRUE, fill = !blank.lines.skip, strip.white = FALSE, blank.lines.skip = TRUE, comment.char = &quot;#&quot;, allowEscapes = FALSE, flush = FALSE, stringsAsFactors = default.stringsAsFactors(), fileEncoding = &quot;&quot;, encoding = &quot;unknown&quot;, text, skipNul = FALSE) read.csv(file, header = TRUE, sep = &quot;,&quot;, quote = &quot;\\&quot;&quot;, dec = &quot;.&quot;, fill = TRUE, comment.char = &quot;&quot;, ...) read.csv2(file, header = TRUE, sep = &quot;;&quot;, quote = &quot;\\&quot;&quot;, dec = &quot;,&quot;, fill = TRUE, comment.char = &quot;&quot;, ...) read.delim(file, header = TRUE, sep = &quot;\\t&quot;, quote = &quot;\\&quot;&quot;, dec = &quot;.&quot;, fill = TRUE, comment.char = &quot;&quot;, ...) read.delim2(file, header = TRUE, sep = &quot;\\t&quot;, quote = &quot;\\&quot;&quot;, dec = &quot;,&quot;, fill = TRUE, comment.char = &quot;&quot;, ...) 变量名是不允许以下划线开头的，同样在数据框里，列名也不推荐使用下划线开头。默认情况下，read.table 都会通过参数 check.names 检查列名的有效性，该参数实际调用了函数 make.names 去检查。如果想尽量保持数据集原来的样子可以设置参数 check.names = FALSE, stringsAsFactors = FALSE。 默认情形下，read.table 还会将字符串转化为因子变量，这是 R 的历史原因，作为一门统计学家的必备语言，在统计模型中，字符常用来描述类别，而类别变量在 R 环境中常用因子类型来表示，而且大量内置的统计模型也是将它们视为因子变量，如 lm 、glm 等 dat1 = read.table(header = TRUE, check.names = TRUE, text = &quot; _a _b _c 1 2 a1 3 4 a2 &quot;) dat1 #&gt; X_a X_b X_c #&gt; 1 1 2 a1 #&gt; 2 3 4 a2 dat2 = read.table(header = TRUE, check.names = FALSE, text = &quot; _a _b _c 1 2 a1 3 4 a2 &quot;) dat2 #&gt; _a _b _c #&gt; 1 1 2 a1 #&gt; 2 3 4 a2 dat3 = read.table(header = TRUE, check.names = FALSE, stringsAsFactors = FALSE, text = &quot; _a _b _c 1 2 a1 3 4 a2 &quot;) dat3 #&gt; _a _b _c #&gt; 1 1 2 a1 #&gt; 2 3 4 a2 1.1.3 readLines readLines(con = stdin(), n = -1L, ok = TRUE, warn = TRUE, encoding = &quot;unknown&quot;, skipNul = FALSE) 让我们折腾一波，读进来又写出去，只有 R 3.5.3 以上才能保持原样的正确输入输出，因为这里有一个之前版本包含的 BUG writeLines(readLines(system.file(&quot;DESCRIPTION&quot;, package = &quot;splines&quot;)), &quot;data/DESCRIPTION&quot;) # 比较一下 identical( readLines(system.file(&quot;DESCRIPTION&quot;, package = &quot;splines&quot;)), readLines(&quot;data/DESCRIPTION&quot;) ) #&gt; [1] TRUE 这次我们创建一个真的临时文件，因为重新启动 R 这个文件和文件夹就没有了，回收掉了 fil &lt;- tempfile(fileext = &quot;.data&quot;) cat(&quot;TITLE extra line&quot;, &quot;2 3 5 7&quot;, &quot;&quot;, &quot;11 13 17&quot;, file = fil, sep = &quot;\\n&quot;) fil #&gt; [1] &quot;/tmp/RtmpLmW61g/file486bd74aa1.data&quot; 设置参数 n = -1 表示将文件 fil 的内容从头读到尾 readLines(fil, n = -1) #&gt; [1] &quot;TITLE extra line&quot; &quot;2 3 5 7&quot; &quot;&quot; #&gt; [4] &quot;11 13 17&quot; 作为拥有良好习惯的 R 用户，这种垃圾文件最好用后即焚 unlink(fil) # tidy up 再举个例子，我们创建一个新的临时文件 fil，文件内容只有 cat(&quot;123\\nabc&quot;) #&gt; 123 #&gt; abc fil &lt;- tempfile(&quot;test&quot;) cat(&quot;123\\nabc\\n&quot;, file = fil, append = TRUE) fil #&gt; [1] &quot;/tmp/RtmpLmW61g/test4834174019&quot; readLines(fil) #&gt; [1] &quot;123&quot; &quot;abc&quot; 这次读取文件的过程给出了警告，原因是 fil 没有以空行结尾，warn = TRUE 表示这种情况要给出警告，如果设置参数 warn = FALSE 就没有警告。我们还是建议大家尽量遵循规范。 再举一个例子，从一个连接读取数据，建立连接的方式有很多，参见 ?file，下面设置参数 blocking con &lt;- file(fil, &quot;r&quot;, blocking = FALSE) readLines(con) #&gt; [1] &quot;123&quot; &quot;abc&quot; cat(&quot; def\\n&quot;, file = fil, append = TRUE) readLines(con) #&gt; [1] &quot; def&quot; # 关闭连接 close(con) # 清理垃圾文件 unlink(fil) 1.1.4 readRDS 序列化数据操作，Mark Klik 开发的 fst 和 Travers Ching 开发的 qs， Hadley Wickham 开发的 feather 包实现跨语言环境快速的读写数据 表 1.1: fst 序列化数据框对象性能比较 BaseR、 data.table 和 feather10 Method Format Time (ms) Size (MB) Speed (MB/s) N readRDS bin 1577 1000 633 112 saveRDS bin 2042 1000 489 112 fread csv 2925 1038 410 232 fwrite csv 2790 1038 358 241 read_feather bin 3950 813 253 112 write_feather bin 1820 813 549 112 read_fst bin 457 303 2184 282 write_fst bin 314 303 3180 291 目前比较好的是 qs 和 fst 包 1.2 其它数据格式 来自其它格式的数据形式，如 JSON、XML、YAML 需要转化清理成 R 中数据框的形式 data.frame Data Rectangling with jq Mongolite User Manual introduction to using MongoDB with the mongolite client in R jsonlite 读取 *.json 格式的文件，jsonlite::write_json 函数将 R对象保存为 JSON 文件，jsonlite::fromJSON 将 json 字符串或文件转化为 R 对象，jsonlite::toJSON 函数正好与之相反 library(jsonlite) # 从 json 格式的文件导入 # jsonlite::read_json(path = &quot;path/to/filename.json&quot;) # A JSON array of primitives json &lt;- &#39;[&quot;Mario&quot;, &quot;Peach&quot;, null, &quot;Bowser&quot;]&#39; # 简化为原子向量atomic vector fromJSON(json) #&gt; [1] &quot;Mario&quot; &quot;Peach&quot; NA &quot;Bowser&quot; # 默认返回一个列表 fromJSON(json, simplifyVector = FALSE) #&gt; [[1]] #&gt; [1] &quot;Mario&quot; #&gt; #&gt; [[2]] #&gt; [1] &quot;Peach&quot; #&gt; #&gt; [[3]] #&gt; NULL #&gt; #&gt; [[4]] #&gt; [1] &quot;Bowser&quot; yaml 包读取 *.yml 格式文件，返回一个列表，yaml::write_yaml 函数将 R 对象写入 yaml 格式 library(yaml) yaml::read_yaml(file = &#39;_bookdown.yml&#39;) #&gt; $delete_merged_file #&gt; [1] TRUE #&gt; #&gt; $language #&gt; $language$label #&gt; $language$label$fig #&gt; [1] &quot;图 &quot; #&gt; #&gt; $language$label$tab #&gt; [1] &quot;表 &quot; #&gt; #&gt; #&gt; $language$ui #&gt; $language$ui$edit #&gt; [1] &quot;编辑&quot; #&gt; #&gt; $language$ui$chapter_name #&gt; [1] &quot;第 &quot; &quot; 章&quot; #&gt; #&gt; #&gt; #&gt; $output_dir #&gt; [1] &quot;_book&quot; #&gt; #&gt; $new_session #&gt; [1] TRUE #&gt; #&gt; $before_chapter_script #&gt; [1] &quot;_common.R&quot; #&gt; #&gt; $rmd_files #&gt; [1] &quot;index.Rmd&quot; &quot;preface.Rmd&quot; &quot;dm-import-export.Rmd&quot; #&gt; [4] &quot;dm-base-r.Rmd&quot; &quot;dm-dplyr.Rmd&quot; &quot;99-references.Rmd&quot; 表 1.2: 导入来自其它数据分析软件产生的数据集 统计软件 R函数 R包 ERSI ArcGIS read.shapefile shapefiles Matlab readMat R.matlab minitab read.mtp foreign SAS (permanent data) read.ssd foreign SAS (XPORT format) read.xport foreign SPSS read.spss foreign Stata read.dta foreign Systat read.systat foreign Octave read.octave foreign 表 1.3: 导入来自其它格式的数据集 文件格式 R函数 R包 列联表数据 read.ftable stats 二进制数据 readBin base 字符串数据 readChar base 剪贴板数据 readClipboard utils read.dcf 函数读取 Debian 控制格式文件，这种类型的文件以人眼可读的形式在存储数据，如 R 包的 DESCRIPTION 文件或者包含所有 CRAN 上 R 包描述的文件 https://cran.r-project.org/src/contrib/PACKAGES x &lt;- read.dcf(file = system.file(&quot;DESCRIPTION&quot;, package = &quot;splines&quot;), fields = c(&quot;Package&quot;, &quot;Version&quot;, &quot;Title&quot;)) x #&gt; Package Version Title #&gt; [1,] &quot;splines&quot; &quot;3.6.1&quot; &quot;Regression Spline Functions and Classes&quot; 最后要提及拥有瑞士军刀之称的 rio 包，它集合了当前 R 可以读取的所有统计分析软件导出的数据。 1.3 导入大数据集 在不使用数据库的情况下，从命令行导入大数据集，如几百 M 或几个 G 的 csv 文件。利用 data.table 包的 fread 去读取 https://stackoverflow.com/questions/1727772/ 1.4 从数据库导入 Hands-On Programming with R 数据读写章节11 以及 R, Databases and Docker 将大量的 txt 文本存进 MySQL 数据库中，通过操作数据库来聚合文本，极大降低内存消耗12，而 ODBC 与 DBI 包是其它数据库接口的基础，knitr 提供了一个支持 SQL 代码的引擎，它便是基于 DBI，因此可以在 R Markdown 文档中直接使用 SQL 代码块13。这里制作一个归纳表格，左边数据库右边对应其 R 接口，两边都包含链接，如表 1.4 所示 表 1.4: 数据库接口 数据库 官网 R接口 开发仓 MySQL https://www.mysql.com/ RMySQL https://github.com/r-dbi/RMySQL SQLite https://www.sqlite.org RSQLite https://github.com/r-dbi/RSQLite PostgreSQL https://www.postgresql.org/ RPostgres https://github.com/r-dbi/RPostgres MariaDB https://mariadb.org/ RMariaDB https://github.com/r-dbi/RMariaDB 1.4.1 PostgreSQL odbc 可以支持很多数据库，下面以连接 PostgreSQL 数据库为例介绍其过程 首先在某台机器上，拉取 PostgreSQL 的 Docker 镜像 docker pull postgres 在 Docker 上运行 PostgreSQL，主机端口号 8181 映射给数据库 PostgreSQL 的默认端口号 5432（或其它你的 DBA 分配给你的端口） docker run --name psql -d -p 8181:5432 -e ROOT=TRUE \\ -e USER=xiangyun -e PASSWORD=cloud postgres 在主机 Ubuntu 上配置 sudo apt-get install unixodbc unixodbc-dev odbc-postgresql 端口 5432 是分配给 PostgreSQL 的默认端口，host 可以是云端的地址，如 你的亚马逊账户下的 PostgreSQL 数据库地址 &lt;ec2-54-83-201-96.compute-1.amazonaws.com&gt;，也可以是本地局域网IP地址，如&lt;192.168.1.200&gt;。通过参数 dbname 连接到指定的 PostgreSQL 数据库，如 Heroku，这里作为演示就以默认的数据库 postgres 为例 查看配置系统文件路径 odbcinst -j unixODBC 2.3.6 DRIVERS............: /etc/odbcinst.ini SYSTEM DATA SOURCES: /etc/odbc.ini FILE DATA SOURCES..: /etc/ODBCDataSources USER DATA SOURCES..: /root/.odbc.ini SQLULEN Size.......: 8 SQLLEN Size........: 8 SQLSETPOSIROW Size.: 8 不推荐修改全局配置文件，可设置 ODBCSYSINI 环境变量指定配置文件路径，如 ODBCSYSINI=~/ODBC http://www.unixodbc.org/odbcinst.html 安装完驱动程序，/etc/odbcinst.ini 文件内容自动更新，我们可以不必修改，如果你想自定义不妨手动修改，我们查看在 R 环境中注册的数据库，可以看到 PostgreSQL 的驱动已经配置好 odbc::odbcListDrivers() name attribute value 1 PostgreSQL ANSI Description PostgreSQL ODBC driver (ANSI version) 2 PostgreSQL ANSI Driver psqlodbca.so 3 PostgreSQL ANSI Setup libodbcpsqlS.so 4 PostgreSQL ANSI Debug 0 5 PostgreSQL ANSI CommLog 1 6 PostgreSQL ANSI UsageCount 1 7 PostgreSQL Unicode Description PostgreSQL ODBC driver (Unicode version) 8 PostgreSQL Unicode Driver psqlodbcw.so 9 PostgreSQL Unicode Setup libodbcpsqlS.so 10 PostgreSQL Unicode Debug 0 11 PostgreSQL Unicode CommLog 1 12 PostgreSQL Unicode UsageCount 1 系统配置文件 /etc/odbcinst.ini 已经包含有 PostgreSQL 的驱动配置，无需再重复配置 [PostgreSQL ANSI] Description=PostgreSQL ODBC driver (ANSI version) Driver=psqlodbca.so Setup=libodbcpsqlS.so Debug=0 CommLog=1 UsageCount=1 [PostgreSQL Unicode] Description=PostgreSQL ODBC driver (Unicode version) Driver=psqlodbcw.so Setup=libodbcpsqlS.so Debug=0 CommLog=1 UsageCount=1 只需将如下内容存放在 ~/.odbc.ini 文件中， [PostgreSQL] Driver = PostgreSQL Unicode Database = postgres Servername = 172.17.0.1 UserName = postgres Password = default Port = 8080 最后，一行命令 DNS 配置连接 https://github.com/r-dbi/odbc 这样就实现了代码中无任何敏感信息，这里为了展示这个配置过程故而把相关信息公开。 注意下面的内容需要在容器中运行， Windows 环境下的配置 PostgreSQL 的驱动有点麻烦就不搞了，意义也不大，现在数据库基本都是跑在 Linux 系统上 docker-machine.exe ip default 可以获得本地 Docker 的 IP，比如 192.168.99.101。 Travis 上 ip addr 可以查看 Docker 的 IP，如 172.17.0.1 library(DBI) con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;postgres&quot;, host = ifelse(is_on_travis, Sys.getenv(&quot;DOCKER_HOST_IP&quot;), &quot;192.168.99.101&quot;), port = 8080, user = &quot;postgres&quot;, password = &quot;default&quot; ) library(DBI) con &lt;- dbConnect(odbc::odbc(), &quot;PostgreSQL&quot;) 列出数据库中的所有表 dbListTables(con) #&gt; character(0) 第一次启动从 Docker Hub 上下载的镜像，默认的数据库是 postgres 里面没有任何表，所以将 R 环境中的 mtcars 数据集写入 postgres 数据库 将数据集 mtcars 写入 PostgreSQL 数据库中，基本操作，写入表的操作也不能缓存，即不能缓存数据库中的表 mtcars dbWriteTable(con, &quot;mtcars&quot;, mtcars, overwrite = TRUE) 现在可以看到数据表 mtcars 的各个字段 dbListFields(con, &quot;mtcars&quot;) #&gt; [1] &quot;row_names&quot; &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; #&gt; [6] &quot;drat&quot; &quot;wt&quot; &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; #&gt; [11] &quot;gear&quot; &quot;carb&quot; 最后执行一条 SQL 语句 res &lt;- dbSendQuery(con, &quot;SELECT * FROM mtcars WHERE cyl = 4&quot;) # 发送 SQL 语句 dbFetch(res) # 获取查询结果 #&gt; row_names mpg cyl disp hp drat wt qsec vs am gear carb #&gt; 1 Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 #&gt; 2 Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 #&gt; 3 Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 #&gt; 4 Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 #&gt; 5 Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 #&gt; 6 Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 #&gt; 7 Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 #&gt; 8 Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 #&gt; 9 Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 #&gt; 10 Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 #&gt; 11 Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 dbClearResult(res) # 清理查询通道 或者一条命令搞定 dbGetQuery(con, &quot;SELECT * FROM mtcars WHERE cyl = 4&quot;) #&gt; row_names mpg cyl disp hp drat wt qsec vs am gear carb #&gt; 1 Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 #&gt; 2 Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 #&gt; 3 Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 #&gt; 4 Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 #&gt; 5 Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 #&gt; 6 Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 #&gt; 7 Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 #&gt; 8 Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 #&gt; 9 Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 #&gt; 10 Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 #&gt; 11 Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 再复杂一点的 SQL 查询操作 dbGetQuery(con, &quot;SELECT cyl, AVG(mpg) AS mpg FROM mtcars GROUP BY cyl ORDER BY cyl&quot;) #&gt; cyl mpg #&gt; 1 4 26.66364 #&gt; 2 6 19.74286 #&gt; 3 8 15.10000 aggregate(mpg ~ cyl, data = mtcars, mean) #&gt; cyl mpg #&gt; 1 4 26.66364 #&gt; 2 6 19.74286 #&gt; 3 8 15.10000 得益于 knitr (Xie 2015) 开发的钩子，这里直接写 SQL 语句块，打印出来见表 1.5，值得注意的是 SQL 代码块不能启用缓存，数据库连接通道也不能缓存，如果数据库中还没有写入表，那么写入表的操作也不能缓存 SELECT cyl, AVG(mpg) AS mpg FROM mtcars GROUP BY cyl ORDER BY cyl 表 1.5: 表格标题 cyl mpg 4 26.66364 6 19.74286 8 15.10000 如果将查询结果导出到变量，在 Chunk 设置 output.var = \"agg_cyl\" 可以使用缓存，下面将 mpg 按 cyl 分组聚合的结果打印出来 agg_cyl #&gt; cyl mpg #&gt; 1 4 26.66364 #&gt; 2 6 19.74286 #&gt; 3 8 15.10000 这种基于 odbc 的方式的好处就不需要再安装 R 包 RPostgres 和相关系统依赖，最后关闭连接通道 dbDisconnect(con) 1.4.2 MySQL MySQL 是一个很常见，应用也很广泛的数据库，数据分析的常见环境是在一个R Notebook 里，我们可以在正文之前先设定数据库连接信息 ```{r setup} library(DBI) # 指定数据库连接信息 db &lt;- dbConnect(RMySQL::MySQL(), dbname = &#39;dbtest&#39;, username = &#39;user_test&#39;, password = &#39;password&#39;, host = &#39;10.10.101.10&#39;, port = 3306 ) # 创建默认连接 knitr::opts_chunk$set(connection = &#39;db&#39;) # 设置字符编码，以免中文查询乱码 DBI::dbSendQuery(db, &#39;SET NAMES utf8&#39;) # 设置日期变量，以运用在SQL中 idate &lt;- &#39;2019-05-03&#39; ``` SQL 代码块中使用 R 环境中的变量，并将查询结果输出为R环境中的数据框 ```{sql, output.var=&#39;data_output&#39;} SELECT * FROM user_table where date_format(created_date,&#39;%Y-%m-%d&#39;)&gt;=?idate ``` 以上代码会将 SQL 的运行结果存在 data_output 这是数据库中，idate 取之前设置的日期2019-05-03，user_table 是 MySQL 数据库中的表名，created_date 是创建user_table时，指定的日期名。 如果 SQL 比较长，为了代码美观，把带有变量的 SQL 保存为demo.sql脚本，只需要在 SQL 的 chunk 中直接读取 SQL 文件14。 ```{sql, code=readLines(&#39;demo.sql&#39;), output.var=&#39;data_output&#39;} ``` 如果我们需要每天或者按照指定的日期重复地运行这个 R Markdown 文件，可以在 YAML 部分引入参数15 --- params: date: &quot;2019-05-03&quot; # 参数化日期 --- ```{r setup, include=FALSE} idate = params$date # 将参数化日期传递给 idate 变量 ``` 我们将这个 Rmd 文件命名为 MyDocument.Rmd，运行这个文件可以从 R 控制台执行或在 RStudio 点击 knit。 rmarkdown::render(&quot;MyDocument.Rmd&quot;, params = list( date = &quot;2019-05-03&quot; )) 如果在文档的 YAML 位置已经指定日期，这里可以不指定。注意在这里设置日期会覆盖 YAML 处指定的参数值，这样做的好处是可以批量化操作。 1.4.3 Spark 当数据分析报告遇上 Spark 时，就需要 SparkR、 sparklyr、 arrow 或 rsparking 接口了， Javier Luraschi 写了一本书 The R in Spark: Learning Apache Spark with R 详细介绍了相关扩展和应用 首先安装 sparklyr 包，RStudio 公司 Javier Lurasch 开发了 sparklyr 包，作为 Spark 与 R 语言之间的接口，安装完 sparklyr 包，还是需要 Spark 和 Hadoop 环境 install.packages(&#39;sparklyr&#39;) library(sparklyr) spark_install() # Installing Spark 2.4.0 for Hadoop 2.7 or later. # Downloading from: # - &#39;https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz&#39; # Installing to: # - &#39;~/spark/spark-2.4.0-bin-hadoop2.7&#39; # trying URL &#39;https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz&#39; # Content type &#39;application/x-gzip&#39; length 227893062 bytes (217.3 MB) # ================================================== # downloaded 217.3 MB # # Installation complete. 既然 sparklyr 已经安装了 Spark 和 Hadoop 环境，安装 SparkR 后，只需配置好路径，就可以加载 SparkR 包 install.packages(&#39;SparkR&#39;) if (nchar(Sys.getenv(&quot;SPARK_HOME&quot;)) &lt; 1) { Sys.setenv(SPARK_HOME = &quot;~/spark/spark-2.4.0-bin-hadoop2.7&quot;) } library(SparkR, lib.loc = c(file.path(Sys.getenv(&quot;SPARK_HOME&quot;), &quot;R&quot;, &quot;lib&quot;))) sparkR.session(master = &quot;local[*]&quot;, sparkConfig = list(spark.driver.memory = &quot;2g&quot;)) rscala 架起了 R 和 Scala 两门语言之间交流的桥梁，使得彼此之间可以互相调用 是否存在这样的可能， Spark 提供了大量的 MLib 库的调用接口，R 的功能支持是最少的，Java/Scala 是原生的，那么要么自己开发新的功能整合到 SparkR 中，要么借助 rscala 将 scala 接口代码封装进来 1.5 批量导入数据 library(tidyverse) read_list &lt;- function(list_of_datasets, read_func) { read_and_assign &lt;- function(dataset, read_func) { dataset_name &lt;- as.name(dataset) dataset_name &lt;- read_func(dataset) } # invisible is used to suppress the unneeded output output &lt;- invisible( sapply(list_of_datasets, read_and_assign, read_func = read_func, simplify = FALSE, USE.NAMES = TRUE ) ) # Remove the extension at the end of the data set names names_of_datasets &lt;- c(unlist(strsplit(list_of_datasets, &quot;[.]&quot;))[c(T, F)]) names(output) &lt;- names_of_datasets return(output) } 批量导入文件扩展名为 .csv 的数据文件，即逗号分割的文件 data_files &lt;- list.files(path = &quot;path/to/csv/dir&quot;,pattern = &quot;.csv&quot;, full.names = TRUE) print(data_files) 相比于 Base R 提供的 read.csv 函数，使用 readr 包的 read_csv 函数可以更快地读取csv格式文件，特别是在读取GB级数据文件时，效果特别明显。 list_of_data_sets &lt;- read_list(data_files, readr::read_csv) 使用 tibble 包的glimpse函数可以十分方便地对整个数据集有一个大致的了解，展示方式和信息量相当于 str 加 head 函数 tibble::glimpse(list_of_data_sets) 1.6 批量导出数据 假定我们有一个列表，其每个元素都是一个数据框，现在要把每个数据框分别存入 xlsx 表的工作薄中，以 mtcars 数据集为例，将其按分类变量 cyl 分组拆分，获得一个列表 list dat &lt;- split(mtcars, mtcars$cyl) dat #&gt; $`4` #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 #&gt; Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 #&gt; Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 #&gt; Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 #&gt; Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 #&gt; Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 #&gt; Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 #&gt; Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 #&gt; Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 #&gt; Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 #&gt; Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 #&gt; #&gt; $`6` #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 #&gt; Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 #&gt; Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 #&gt; Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 #&gt; Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 #&gt; Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 #&gt; Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 #&gt; #&gt; $`8` #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 #&gt; Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 #&gt; Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 #&gt; Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 #&gt; Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 #&gt; Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 #&gt; Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 #&gt; Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 #&gt; Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 #&gt; AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 #&gt; Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 #&gt; Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 #&gt; Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 #&gt; Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 将 xlsx 表格初始化，创建空白的工作薄， openxlsx 包不依赖 Java 环境，读写效率也高 ## 加载 openxlsx 包 library(openxlsx) ## 创建空白的工作薄 wb &lt;- createWorkbook() 将列表里的每张表分别存入 xlsx 表格的每个 worksheet，worksheet 的名字就是分组变量的名字 Map(function(data, name){ addWorksheet(wb, name) writeData(wb, name, data) }, dat, names(dat)) 最后保存数据到磁盘，见图 1.1 saveWorkbook(wb, file = &quot;data/matcars.xlsx&quot;, overwrite = TRUE) 图 1.1: 批量导出数据 处理 Excel 2003 (XLS) 和 Excel 2007 (XLSX) 文件还可以使用 WriteXLS 包，不过它依赖于 Perl，另一个 R 包 xlsx 与之功能类似，依赖 Java 环境。Jennifer Bryan 和 Hadley Wickham 开发的 readxl 包和 Jeroen Ooms 开发的 writexl 包专门处理 xlsx 格式并且无任何系统依赖 1.7 导出数据 1.7.1 导出运行结果 capture.output(..., file = NULL, append = FALSE, type = c(&quot;output&quot;, &quot;message&quot;), split = FALSE) capture.output 将一段R代码执行结果，保存到文件，参数为表达式。capture.output 和 sink 的关系相当于 with 和 attach 的关系。 glmout &lt;- capture.output(summary(glm(case ~ spontaneous + induced, data = infert, family = binomial() )), file = &quot;data/capture.txt&quot;) capture.output(1 + 1, 2 + 2) #&gt; [1] &quot;[1] 2&quot; &quot;[1] 4&quot; capture.output({ 1 + 1 2 + 2 }) #&gt; [1] &quot;[1] 4&quot; sink 函数将控制台输出结果保存到文件，只将 outer 函数运行的结果保存到 ex-sink.txt 文件，outer 函数计算的是直积，在这里相当于 seq(10) %*% t(seq(10))，而在 R 语言中，更加有效的计算方式是 tcrossprod(seq(10),seq(10)) sink(&quot;data/ex-sink.txt&quot;) i &lt;- 1:10 outer(i, i, &quot;*&quot;) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] #&gt; [1,] 1 2 3 4 5 6 7 8 9 10 #&gt; [2,] 2 4 6 8 10 12 14 16 18 20 #&gt; [3,] 3 6 9 12 15 18 21 24 27 30 #&gt; [4,] 4 8 12 16 20 24 28 32 36 40 #&gt; [5,] 5 10 15 20 25 30 35 40 45 50 #&gt; [6,] 6 12 18 24 30 36 42 48 54 60 #&gt; [7,] 7 14 21 28 35 42 49 56 63 70 #&gt; [8,] 8 16 24 32 40 48 56 64 72 80 #&gt; [9,] 9 18 27 36 45 54 63 72 81 90 #&gt; [10,] 10 20 30 40 50 60 70 80 90 100 sink() 1.7.2 导出数据对象 load(file, envir = parent.frame(), verbose = FALSE) save(..., list = character(), file = stop(&quot;&#39;file&#39; must be specified&quot;), ascii = FALSE, version = NULL, envir = parent.frame(), compress = isTRUE(!ascii), compression_level, eval.promises = TRUE, precheck = TRUE) save.image(file = &quot;.RData&quot;, version = NULL, ascii = FALSE, compress = !ascii, safe = TRUE) load 和save 函数加载或保存包含工作环境信息的数据对象，save.image 保存当前工作环境到磁盘，即保存工作空间中所有数据对象，数据格式为 .RData，即相当于 save(list = ls(all.names = TRUE), file = &quot;.RData&quot;, envir = .GlobalEnv) dump 保存数据对象 AirPassengers 到文件 AirPassengers.txt，文件内容是 R 命令，可把AirPassengers.txt看作代码文档执行，dput 保存数据对象内容到文件AirPassengers.dat，文件中不包含变量名 AirPassengers。注意到 dump 输入是一个字符串，而 dput 要求输入数据对象的名称，source 函数与 dump 对应，而 dget 与 dput对应。 # 加载数据 data(AirPassengers, package = &quot;datasets&quot;) # 将数据以R代码块的形式保存到文件 dump(&#39;AirPassengers&#39;, file = &#39;data/AirPassengers.txt&#39;) # source(file = &#39;data/AirPassengers.txt&#39;) 接下来，我们读取 AirPassengers.txt 的文件内容，可见它是一段完整的 R 代码，可以直接复制到 R 的控制台中运行，并且得到一个与原始 AirPassengers 变量一样的结果 cat(readLines(&#39;data/AirPassengers.txt&#39;), sep = &quot;\\n&quot;) #&gt; AirPassengers &lt;- #&gt; structure(c(112, 118, 132, 129, 121, 135, 148, 148, 136, 119, #&gt; 104, 118, 115, 126, 141, 135, 125, 149, 170, 170, 158, 133, 114, #&gt; 140, 145, 150, 178, 163, 172, 178, 199, 199, 184, 162, 146, 166, #&gt; 171, 180, 193, 181, 183, 218, 230, 242, 209, 191, 172, 194, 196, #&gt; 196, 236, 235, 229, 243, 264, 272, 237, 211, 180, 201, 204, 188, #&gt; 235, 227, 234, 264, 302, 293, 259, 229, 203, 229, 242, 233, 267, #&gt; 269, 270, 315, 364, 347, 312, 274, 237, 278, 284, 277, 317, 313, #&gt; 318, 374, 413, 405, 355, 306, 271, 306, 315, 301, 356, 348, 355, #&gt; 422, 465, 467, 404, 347, 305, 336, 340, 318, 362, 348, 363, 435, #&gt; 491, 505, 404, 359, 310, 337, 360, 342, 406, 396, 420, 472, 548, #&gt; 559, 463, 407, 362, 405, 417, 391, 419, 461, 472, 535, 622, 606, #&gt; 508, 461, 390, 432), .Tsp = c(1949, 1960.91666666667, 12), class = &quot;ts&quot;) dput 函数类似 dump 函数，保存数据对象到磁盘文件 # 将 R 对象保存/导出到磁盘 dput(AirPassengers, file = &#39;data/AirPassengers.dat&#39;) AirPassengers Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 1949 112 118 132 129 121 135 148 148 136 119 104 118 1950 115 126 141 135 125 149 170 170 158 133 114 140 1951 145 150 178 163 172 178 199 199 184 162 146 166 1952 171 180 193 181 183 218 230 242 209 191 172 194 1953 196 196 236 235 229 243 264 272 237 211 180 201 1954 204 188 235 227 234 264 302 293 259 229 203 229 1955 242 233 267 269 270 315 364 347 312 274 237 278 1956 284 277 317 313 318 374 413 405 355 306 271 306 1957 315 301 356 348 355 422 465 467 404 347 305 336 1958 340 318 362 348 363 435 491 505 404 359 310 337 1959 360 342 406 396 420 472 548 559 463 407 362 405 1960 417 391 419 461 472 535 622 606 508 461 390 432 # dget 作用与 dput 相反 AirPassengers2 &lt;- dget(file = &#39;data/AirPassengers.dat&#39;) AirPassengers2 Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 1949 112 118 132 129 121 135 148 148 136 119 104 118 1950 115 126 141 135 125 149 170 170 158 133 114 140 1951 145 150 178 163 172 178 199 199 184 162 146 166 1952 171 180 193 181 183 218 230 242 209 191 172 194 1953 196 196 236 235 229 243 264 272 237 211 180 201 1954 204 188 235 227 234 264 302 293 259 229 203 229 1955 242 233 267 269 270 315 364 347 312 274 237 278 1956 284 277 317 313 318 374 413 405 355 306 271 306 1957 315 301 356 348 355 422 465 467 404 347 305 336 1958 340 318 362 348 363 435 491 505 404 359 310 337 1959 360 342 406 396 420 472 548 559 463 407 362 405 1960 417 391 419 461 472 535 622 606 508 461 390 432 同样地，现在我们观察 dput 函数保存的文件 AirPassengers.dat 内容，和dump 函数保存的文件 AirPassengers.txt相比，就缺一个赋值变量 cat(readLines(&#39;data/AirPassengers.dat&#39;), sep = &quot;\\n&quot;) structure(c(112, 118, 132, 129, 121, 135, 148, 148, 136, 119, 104, 118, 115, 126, 141, 135, 125, 149, 170, 170, 158, 133, 114, 140, 145, 150, 178, 163, 172, 178, 199, 199, 184, 162, 146, 166, 171, 180, 193, 181, 183, 218, 230, 242, 209, 191, 172, 194, 196, 196, 236, 235, 229, 243, 264, 272, 237, 211, 180, 201, 204, 188, 235, 227, 234, 264, 302, 293, 259, 229, 203, 229, 242, 233, 267, 269, 270, 315, 364, 347, 312, 274, 237, 278, 284, 277, 317, 313, 318, 374, 413, 405, 355, 306, 271, 306, 315, 301, 356, 348, 355, 422, 465, 467, 404, 347, 305, 336, 340, 318, 362, 348, 363, 435, 491, 505, 404, 359, 310, 337, 360, 342, 406, 396, 420, 472, 548, 559, 463, 407, 362, 405, 417, 391, 419, 461, 472, 535, 622, 606, 508, 461, 390, 432), .Tsp = c(1949, 1960.91666666667, 12), class = &quot;ts&quot;) 1.8 运行环境 xfun::session_info(c(&quot;jsonlite&quot;, &quot;yaml&quot;, &quot;odbc&quot;)) #&gt; R version 3.6.1 (2019-07-05) #&gt; Platform: x86_64-pc-linux-gnu (64-bit) #&gt; Running under: Debian GNU/Linux 10 (buster) #&gt; #&gt; Locale: #&gt; LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C #&gt; LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 #&gt; LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 #&gt; LC_PAPER=en_US.UTF-8 LC_NAME=C #&gt; LC_ADDRESS=C LC_TELEPHONE=C #&gt; LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C #&gt; #&gt; Package version: #&gt; assertthat_0.2.1 backports_1.1.4 BH_1.69.0.1 #&gt; bit_1.1.14 bit64_0.9.7 blob_1.2.0 #&gt; DBI_1.0.0 digest_0.6.20 ellipsis_0.2.0.1 #&gt; glue_1.3.1 graphics_3.6.1 grDevices_3.6.1 #&gt; hms_0.5.0 jsonlite_1.6 magrittr_1.5 #&gt; methods_3.6.1 odbc_1.1.6 pkgconfig_2.0.2 #&gt; prettyunits_1.0.2 Rcpp_1.0.2 rlang_0.4.0 #&gt; stats_3.6.1 tools_3.6.1 utils_3.6.1 #&gt; vctrs_0.2.0 yaml_2.2.0 zeallot_0.1.0 参考文献 "],
["dm-base-r.html", "第 2 章 数据操作手 2.1 查看数据 2.2 数据变形 2.3 数据转换 2.4 提取子集 2.5 按列排序 2.6 数据拆分 2.7 数据合并 2.8 数据去重 2.9 数据聚合 2.10 表格统计 2.11 索引访问 2.12 多维数组 2.13 其它操作 2.14 运行环境", " 第 2 章 数据操作手 参考 Data Manipulation With R (Spector 2008) 重新捋一遍本章 本章的操作对象是 data.frame 介绍 Base R 提供的数据操作，关于采用 Base R 还是 tidyverse 做数据操作的 讨论 数据操作的动画展示参考 https://github.com/gadenbuie/tidyexplain 提供 Base R 对应的实现 什么是 Base R? Base R 指的是 R 语言/软件的核心组件，由 R Core Team 维护 Pkgs &lt;- sapply(list.files(R.home(&quot;library&quot;)), function(x) packageDescription(pkg = x, fields = &quot;Priority&quot;)) names(Pkgs[Pkgs == &quot;base&quot; &amp; !is.na(Pkgs)]) #&gt; [1] &quot;base&quot; &quot;compiler&quot; &quot;datasets&quot; &quot;graphics&quot; &quot;grDevices&quot; #&gt; [6] &quot;grid&quot; &quot;methods&quot; &quot;parallel&quot; &quot;splines&quot; &quot;stats&quot; #&gt; [11] &quot;stats4&quot; &quot;tcltk&quot; &quot;tools&quot; &quot;utils&quot; names(Pkgs[Pkgs == &quot;recommended&quot; &amp; !is.na(Pkgs)]) #&gt; character(0) 数据变形，分组统计聚合等，用以作为模型的输入，绘图的对象，操作的数据对象是数据框(data.frame)类型的，而且如果没有特别说明，文中出现的数据集都是 Base R 内置的，第三方 R 包或者来源于网上的数据集都会加以说明。 2.1 查看数据 查看属性 str(iris) #&gt; &#39;data.frame&#39;: 150 obs. of 5 variables: #&gt; $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... #&gt; $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... #&gt; $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... #&gt; $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... #&gt; $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1.. 查看部分数据集 head(iris, 5) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3.0 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5.0 3.6 1.4 0.2 setosa tail(iris, 5) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 146 6.7 3.0 5.2 2.3 virginica #&gt; 147 6.3 2.5 5.0 1.9 virginica #&gt; 148 6.5 3.0 5.2 2.0 virginica #&gt; 149 6.2 3.4 5.4 2.3 virginica #&gt; 150 5.9 3.0 5.1 1.8 virginica 查看文件前（后）5行 head -n 5 test.csv tail -n 5 test.csv 对象的类型，存储方式 class(iris) #&gt; [1] &quot;data.frame&quot; mode(iris) #&gt; [1] &quot;list&quot; typeof(iris) #&gt; [1] &quot;list&quot; 查看对象在R环境中所占空间的大小 object.size(iris) #&gt; 7256 bytes object.size(letters) #&gt; 1712 bytes object.size(ls) #&gt; 89904 bytes format(object.size(library), units = &quot;auto&quot;) #&gt; [1] &quot;1.8 Mb&quot; 2.2 数据变形 重复测量数据的变形 Reshape Grouped Data，将宽格式 wide 的数据框变长格式 long的，反之也行。reshape 还支持正则表达式 str(Indometh) #&gt; Classes &#39;nfnGroupedData&#39;, &#39;nfGroupedData&#39;, &#39;groupedData&#39; and &#39;data.frame&#39;: 66 obs. of 3 variables: #&gt; $ Subject: Ord.factor w/ 6 levels &quot;1&quot;&lt;&quot;4&quot;&lt;&quot;2&quot;&lt;&quot;5&quot;&lt;..: 1 1 1 1 1 1 1 1 1 .. #&gt; $ time : num 0.25 0.5 0.75 1 1.25 2 3 4 5 6 ... #&gt; $ conc : num 1.5 0.94 0.78 0.48 0.37 0.19 0.12 0.11 0.08 0.07 ... #&gt; - attr(*, &quot;formula&quot;)=Class &#39;formula&#39; language conc ~ time | Subject #&gt; .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; #&gt; - attr(*, &quot;labels&quot;)=List of 2 #&gt; ..$ x: chr &quot;Time since drug administration&quot; #&gt; ..$ y: chr &quot;Indomethacin concentration&quot; #&gt; - attr(*, &quot;units&quot;)=List of 2 #&gt; ..$ x: chr &quot;(hr)&quot; #&gt; ..$ y: chr &quot;(mcg/ml)&quot; summary(Indometh) #&gt; Subject time conc #&gt; 1:11 Min. :0.250 Min. :0.0500 #&gt; 4:11 1st Qu.:0.750 1st Qu.:0.1100 #&gt; 2:11 Median :2.000 Median :0.3400 #&gt; 5:11 Mean :2.886 Mean :0.5918 #&gt; 6:11 3rd Qu.:5.000 3rd Qu.:0.8325 #&gt; 3:11 Max. :8.000 Max. :2.7200 # 长的变宽 wide &lt;- reshape(Indometh, v.names = &quot;conc&quot;, idvar = &quot;Subject&quot;, timevar = &quot;time&quot;, direction = &quot;wide&quot; ) wide[, 1:6] #&gt; Subject conc.0.25 conc.0.5 conc.0.75 conc.1 conc.1.25 #&gt; 1 1 1.50 0.94 0.78 0.48 0.37 #&gt; 12 2 2.03 1.63 0.71 0.70 0.64 #&gt; 23 3 2.72 1.49 1.16 0.80 0.80 #&gt; 34 4 1.85 1.39 1.02 0.89 0.59 #&gt; 45 5 2.05 1.04 0.81 0.39 0.30 .... # 宽的变长 reshape(wide, direction = &quot;long&quot;) #&gt; Subject time conc #&gt; 1.0.25 1 0.25 1.50 #&gt; 2.0.25 2 0.25 2.03 #&gt; 3.0.25 3 0.25 2.72 #&gt; 4.0.25 4 0.25 1.85 #&gt; 5.0.25 5 0.25 2.05 .... 宽的格式变成长的格式 https://stackoverflow.com/questions/2185252 长的格式变成宽的格式 https://stackoverflow.com/questions/5890584/ set.seed(45) dat &lt;- data.frame( name = rep(c(&quot;Orange&quot;, &quot;Apple&quot;), each=4), numbers = rep(1:4, 2), value = rnorm(8)) dat #&gt; name numbers value #&gt; 1 Orange 1 0.3407997 #&gt; 2 Orange 2 -0.7033403 #&gt; 3 Orange 3 -0.3795377 #&gt; 4 Orange 4 -0.7460474 #&gt; 5 Apple 1 -0.8981073 #&gt; 6 Apple 2 -0.3347941 #&gt; 7 Apple 3 -0.5013782 #&gt; 8 Apple 4 -0.1745357 reshape(dat, idvar = &quot;name&quot;, timevar = &quot;numbers&quot;, direction = &quot;wide&quot;) #&gt; name value.1 value.2 value.3 value.4 #&gt; 1 Orange 0.3407997 -0.7033403 -0.3795377 -0.7460474 #&gt; 5 Apple -0.8981073 -0.3347941 -0.5013782 -0.1745357 更加复杂的例子， gambia 数据集，重塑的效果是使得个体水平的长格式变为村庄水平的宽格式 # data(gambia, package = &quot;geoR&quot;) # 在线下载数据集 gambia &lt;- read.table( file = paste(&quot;http://www.leg.ufpr.br/lib/exe/fetch.php&quot;, &quot;pessoais:paulojus:mbgbook:datasets:gambia.txt&quot;, sep = &quot;/&quot; ), header = TRUE ) head(gambia) #&gt; x y pos age netuse treated green phc #&gt; 1 349631.3 1458055 1 1783 0 0 40.85 1 #&gt; 2 349631.3 1458055 0 404 1 0 40.85 1 #&gt; 3 349631.3 1458055 0 452 1 0 40.85 1 #&gt; 4 349631.3 1458055 1 566 1 0 40.85 1 #&gt; 5 349631.3 1458055 0 598 1 0 40.85 1 #&gt; 6 349631.3 1458055 1 590 1 0 40.85 1 # Building a &quot;village-level&quot; data frame ind &lt;- paste(&quot;x&quot;, gambia[, 1], &quot;y&quot;, gambia[, 2], sep = &quot;&quot;) village &lt;- gambia[!duplicated(ind), c(1:2, 7:8)] village$prev &lt;- as.vector(tapply(gambia$pos, ind, mean)) head(village) #&gt; x y green phc prev #&gt; 1 349631.3 1458055 40.85 1 0.5151515 #&gt; 34 358543.1 1460112 40.85 1 0.3015873 #&gt; 97 360308.1 1460026 40.10 0 0.4117647 #&gt; 114 363795.7 1496919 40.85 0 0.3333333 #&gt; 138 366400.5 1460248 40.85 0 0.3846154 #&gt; 164 366687.5 1463002 40.85 0 0.3888889 2.3 数据转换 transform 对数据框中的某些列做计算，取对数，将计算的结果单存一列加到数据框中 transform(iris, scale.sl = (max(Sepal.Length) - Sepal.Length) / (max(Sepal.Length) - min(Sepal.Length))) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3.0 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5.0 3.6 1.4 0.2 setosa .... 验证一下 scale.sl 变量的第一个值 (max(iris$Sepal.Length) - 5.1) / (max(iris$Sepal.Length) - min(iris$Sepal.Length)) #&gt; [1] 0.7777778 Warning: This is a convenience function intended for use interactively. For programming it is better to use the standard subsetting arithmetic functions, and in particular the non-standard evaluation of argument transform can have unanticipated consequences. 2.4 提取子集 subset(x, subset, select, drop = FALSE, ...) 参数 subset代表行操作，select 代表列操作，函数 subset 从数据框中提取部分数据 subset(iris, Species == &quot;virginica&quot;) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 101 6.3 3.3 6.0 2.5 virginica #&gt; 102 5.8 2.7 5.1 1.9 virginica #&gt; 103 7.1 3.0 5.9 2.1 virginica #&gt; 104 6.3 2.9 5.6 1.8 virginica #&gt; 105 6.5 3.0 5.8 2.2 virginica .... # summary(iris$Sepal.Length) mean(iris$Sepal.Length) # 且的逻辑 # subset(iris, Species == &quot;virginica&quot; &amp; Sepal.Length &gt; 5.84333) subset(iris, Species == &quot;virginica&quot; &amp; Sepal.Length &gt; mean(Sepal.Length)) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 101 6.3 3.3 6.0 2.5 virginica #&gt; 103 7.1 3.0 5.9 2.1 virginica #&gt; 104 6.3 2.9 5.6 1.8 virginica #&gt; 105 6.5 3.0 5.8 2.2 virginica #&gt; 106 7.6 3.0 6.6 2.1 virginica .... # 在行的子集范围内 subset(iris, Species %in% c(&quot;virginica&quot;, &quot;versicolor&quot;) &amp; Sepal.Length &gt; mean(Sepal.Length)) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 51 7.0 3.2 4.7 1.4 versicolor #&gt; 52 6.4 3.2 4.5 1.5 versicolor #&gt; 53 6.9 3.1 4.9 1.5 versicolor #&gt; 55 6.5 2.8 4.6 1.5 versicolor #&gt; 57 6.3 3.3 4.7 1.6 versicolor .... # 在列的子集内 先选中列 subset(iris, Sepal.Length &gt; mean(Sepal.Length), select = c(&quot;Sepal.Length&quot;, &quot;Species&quot;) ) #&gt; Sepal.Length Species #&gt; 51 7.0 versicolor #&gt; 52 6.4 versicolor #&gt; 53 6.9 versicolor #&gt; 55 6.5 versicolor #&gt; 57 6.3 versicolor .... 高级操作：加入正则表达式筛选 ## sometimes requiring a logical &#39;subset&#39; argument is a nuisance nm &lt;- rownames(state.x77) start_with_M &lt;- nm %in% grep(&quot;^M&quot;, nm, value = TRUE) subset(state.x77, start_with_M, Illiteracy:Murder) #&gt; Illiteracy Life Exp Murder #&gt; Maine 0.7 70.39 2.7 #&gt; Maryland 0.9 70.22 8.5 #&gt; Massachusetts 1.1 71.83 3.3 #&gt; Michigan 0.9 70.63 11.1 #&gt; Minnesota 0.6 72.96 2.3 #&gt; Mississippi 2.4 68.09 12.5 #&gt; Missouri 0.8 70.69 9.3 #&gt; Montana 0.6 70.56 5.0 # 简化 subset(state.x77, subset = grepl(&quot;^M&quot;, rownames(state.x77)), select = Illiteracy:Murder) #&gt; Illiteracy Life Exp Murder #&gt; Maine 0.7 70.39 2.7 #&gt; Maryland 0.9 70.22 8.5 #&gt; Massachusetts 1.1 71.83 3.3 #&gt; Michigan 0.9 70.63 11.1 #&gt; Minnesota 0.6 72.96 2.3 #&gt; Mississippi 2.4 68.09 12.5 #&gt; Missouri 0.8 70.69 9.3 #&gt; Montana 0.6 70.56 5.0 # 继续简化 subset(state.x77, grepl(&quot;^M&quot;, rownames(state.x77)), Illiteracy:Murder) #&gt; Illiteracy Life Exp Murder #&gt; Maine 0.7 70.39 2.7 #&gt; Maryland 0.9 70.22 8.5 #&gt; Massachusetts 1.1 71.83 3.3 #&gt; Michigan 0.9 70.63 11.1 #&gt; Minnesota 0.6 72.96 2.3 #&gt; Mississippi 2.4 68.09 12.5 #&gt; Missouri 0.8 70.69 9.3 #&gt; Montana 0.6 70.56 5.0 警告：这是一个为了交互使用打造的便捷函数。对于编程，最好使用标准的子集函数，如 [，特别地，参数 subset 的非标准计算(non-standard evaluation)16 可能带来意想不到的后果。 使用索引 [ iris[iris$Species == &quot;virginica&quot;, ] #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 101 6.3 3.3 6.0 2.5 virginica #&gt; 102 5.8 2.7 5.1 1.9 virginica #&gt; 103 7.1 3.0 5.9 2.1 virginica #&gt; 104 6.3 2.9 5.6 1.8 virginica #&gt; 105 6.5 3.0 5.8 2.2 virginica .... iris[iris$Species == &quot;virginica&quot; &amp; iris$Sepal.Length &gt; mean(iris$Sepal.Length), ] #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 101 6.3 3.3 6.0 2.5 virginica #&gt; 103 7.1 3.0 5.9 2.1 virginica #&gt; 104 6.3 2.9 5.6 1.8 virginica #&gt; 105 6.5 3.0 5.8 2.2 virginica #&gt; 106 7.6 3.0 6.6 2.1 virginica .... iris[ iris$Species == &quot;virginica&quot; &amp; iris$Sepal.Length &gt; mean(iris$Sepal.Length), c(&quot;Sepal.Length&quot;, &quot;Species&quot;) ] #&gt; Sepal.Length Species #&gt; 101 6.3 virginica #&gt; 103 7.1 virginica #&gt; 104 6.3 virginica #&gt; 105 6.5 virginica #&gt; 106 7.6 virginica .... 2.5 按列排序 在数据框内，根据(order)某一列或几列对行进行排序(sort)，根据鸢尾花(iris)的类别(Species)对萼片(sepal)的长度进行排序，其余的列随之变化 # 对萼片的长度排序 iris[order(iris$Species, iris$Sepal.Length), ] #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 14 4.3 3.0 1.1 0.1 setosa #&gt; 9 4.4 2.9 1.4 0.2 setosa #&gt; 39 4.4 3.0 1.3 0.2 setosa #&gt; 43 4.4 3.2 1.3 0.2 setosa #&gt; 42 4.5 2.3 1.3 0.3 setosa .... # 对花瓣的长度排序 iris[order(iris$Species, iris$Petal.Length), ] #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 23 4.6 3.6 1.0 0.2 setosa #&gt; 14 4.3 3.0 1.1 0.1 setosa #&gt; 15 5.8 4.0 1.2 0.2 setosa #&gt; 36 5.0 3.2 1.2 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa .... # 先对花瓣的宽度排序，再对花瓣的长度排序 iris[order(iris$Petal.Width, iris$Petal.Length), ] #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; 14 4.3 3.0 1.1 0.1 setosa #&gt; 13 4.8 3.0 1.4 0.1 setosa #&gt; 38 4.9 3.6 1.4 0.1 setosa #&gt; 10 4.9 3.1 1.5 0.1 setosa #&gt; 33 5.2 4.1 1.5 0.1 setosa .... sort/ordered 排序， 默认是升序 dd &lt;- data.frame( b = factor(c(&quot;Hi&quot;, &quot;Med&quot;, &quot;Hi&quot;, &quot;Low&quot;), levels = c(&quot;Low&quot;, &quot;Med&quot;, &quot;Hi&quot;), ordered = TRUE ), x = c(&quot;A&quot;, &quot;D&quot;, &quot;A&quot;, &quot;C&quot;), y = c(8, 3, 9, 9), z = c(1, 1, 1, 2) ) str(dd) #&gt; &#39;data.frame&#39;: 4 obs. of 4 variables: #&gt; $ b: Ord.factor w/ 3 levels &quot;Low&quot;&lt;&quot;Med&quot;&lt;&quot;Hi&quot;: 3 2 3 1 #&gt; $ x: chr &quot;A&quot; &quot;D&quot; &quot;A&quot; &quot;C&quot; #&gt; $ y: num 8 3 9 9 #&gt; $ z: num 1 1 1 2 dd[order(-dd[,4], dd[,1]), ] #&gt; b x y z #&gt; 4 Low C 9 2 #&gt; 2 Med D 3 1 #&gt; 1 Hi A 8 1 #&gt; 3 Hi A 9 1 根据变量 z dd[order(dd$z, dd$b), ] #&gt; b x y z #&gt; 2 Med D 3 1 #&gt; 1 Hi A 8 1 #&gt; 3 Hi A 9 1 #&gt; 4 Low C 9 2 2.6 数据拆分 数据拆分通常是按找某一个分类变量分组，分完组就是计算，计算完就把结果按照原来的分组方式合并 ## Notice that assignment form is not used since a variable is being added g &lt;- airquality$Month l &lt;- split(airquality, g) # 分组 l &lt;- lapply(l, transform, Oz.Z = scale(Ozone)) # 计算：按月对 Ozone 标准化 aq2 &lt;- unsplit(l, g) # 合并 head(aq2) #&gt; Ozone Solar.R Wind Temp Month Day Oz.Z #&gt; 1 41 190 7.4 67 5 1 0.7822293 #&gt; 2 36 118 8.0 72 5 2 0.5572518 #&gt; 3 12 149 12.6 74 5 3 -0.5226399 #&gt; 4 18 313 11.5 62 5 4 -0.2526670 #&gt; 5 NA NA 14.3 56 5 5 NA #&gt; 6 28 NA 14.9 66 5 6 0.1972879 tapply 自带分组的功能，按月份 Month 对 Ozone 中心标准化，其返回一个列表 with(airquality, tapply(Ozone, Month, scale)) #&gt; $`5` #&gt; [,1] #&gt; [1,] 0.78222929 #&gt; [2,] 0.55725184 #&gt; [3,] -0.52263993 #&gt; [4,] -0.25266698 #&gt; [5,] NA #&gt; [6,] 0.19728792 #&gt; [7,] -0.02768953 #&gt; [8,] -0.20767149 .... 上面的过程等价于 do.call(&quot;rbind&quot;, lapply(split(airquality, airquality$Month), transform, Oz.Z = scale(Ozone))) #&gt; Ozone Solar.R Wind Temp Month Day Oz.Z #&gt; 5.1 41 190 7.4 67 5 1 0.782229293 #&gt; 5.2 36 118 8.0 72 5 2 0.557251841 #&gt; 5.3 12 149 12.6 74 5 3 -0.522639926 #&gt; 5.4 18 313 11.5 62 5 4 -0.252666984 #&gt; 5.5 NA NA 14.3 56 5 5 NA #&gt; 5.6 28 NA 14.9 66 5 6 0.197287919 #&gt; 5.7 23 299 8.6 65 5 7 -0.027689532 #&gt; 5.8 19 99 13.8 59 5 8 -0.207671494 #&gt; 5.9 8 19 20.1 61 5 9 -0.702621887 .... 由于上面对 Ozone 正态标准化，所以标准化后的 Oz.z 再按月分组计算方差自然每个月都是 1，而均值都是 0。 with(aq2, tapply(Oz.Z, Month, sd, na.rm = TRUE)) #&gt; 5 6 7 8 9 #&gt; 1 1 1 1 1 with(aq2, tapply(Oz.Z, Month, mean, na.rm = TRUE)) #&gt; 5 6 7 8 9 #&gt; -4.240273e-17 1.052760e-16 5.841432e-17 5.898060e-17 2.571709e-17 循着这个思路，我们可以用 tapply 实现分组计算，上面函数 sd 和 mean 完全可以用自定义的更加复杂的函数替代 cut 函数可以将连续型变量划分为分类变量 set.seed(2019) Z &lt;- stats::rnorm(10) cut(Z, breaks = -6:6) #&gt; [1] (0,1] (-1,0] (-2,-1] (0,1] (-2,-1] (0,1] (-1,0] (0,1] #&gt; [9] (-2,-1] (-1,0] #&gt; 12 Levels: (-6,-5] (-5,-4] (-4,-3] (-3,-2] (-2,-1] (-1,0] (0,1] ... (5,6] # labels = FALSE 返回每个数所落的区间位置 cut(Z, breaks = -6:6, labels = FALSE) #&gt; [1] 7 6 5 7 5 7 6 7 5 6 我们还可以指定参数 dig.lab 设置分组的精度，ordered 将分组变量看作是有序的，breaks 传递单个数时，表示分组数，而不是断点 cut(Z, breaks = 3, dig.lab = 4, ordered = TRUE) #&gt; [1] (0.06396,0.9186] (-0.7881,0.06396] (-1.643,-0.7881] #&gt; [4] (0.06396,0.9186] (-1.643,-0.7881] (0.06396,0.9186] #&gt; [7] (-0.7881,0.06396] (0.06396,0.9186] (-1.643,-0.7881] #&gt; [10] (-0.7881,0.06396] #&gt; Levels: (-1.643,-0.7881] &lt; (-0.7881,0.06396] &lt; (0.06396,0.9186] 此时，统计每组的频数，如图 2.1 # 条形图 plot(cut(Z, breaks = -6:6)) # 直方图 hist(Z, breaks = -6:6) 图 2.1: 连续型变量分组统计 在指定分组数的情况下，我们还想获取分组的断点 labs &lt;- levels(cut(Z, 3)) labs #&gt; [1] &quot;(-1.64,-0.788]&quot; &quot;(-0.788,0.064]&quot; &quot;(0.064,0.919]&quot; 用正则表达式抽取断点 cbind( lower = as.numeric(sub(&quot;\\\\((.+),.*&quot;, &quot;\\\\1&quot;, labs)), upper = as.numeric(sub(&quot;[^,]*,([^]]*)\\\\]&quot;, &quot;\\\\1&quot;, labs)) ) #&gt; lower upper #&gt; [1,] -1.640 -0.788 #&gt; [2,] -0.788 0.064 #&gt; [3,] 0.064 0.919 更多相关函数可以参考 findInterval 和 embed tabulate 和 table 有所不同，它表示排列 t(combn(8, 4, tabulate, nbins = 8)) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] #&gt; [1,] 1 1 1 1 0 0 0 0 #&gt; [2,] 1 1 1 0 1 0 0 0 #&gt; [3,] 1 1 1 0 0 1 0 0 #&gt; [4,] 1 1 1 0 0 0 1 0 #&gt; [5,] 1 1 1 0 0 0 0 1 .... 2.7 数据合并 merge 合并两个数据框 authors &lt;- data.frame( ## I(*) : use character columns of names to get sensible sort order surname = I(c(&quot;Tukey&quot;, &quot;Venables&quot;, &quot;Tierney&quot;, &quot;Ripley&quot;, &quot;McNeil&quot;)), nationality = c(&quot;US&quot;, &quot;Australia&quot;, &quot;US&quot;, &quot;UK&quot;, &quot;Australia&quot;), deceased = c(&quot;yes&quot;, rep(&quot;no&quot;, 4)) ) authorN &lt;- within(authors, { name &lt;- surname rm(surname) }) books &lt;- data.frame( name = I(c( &quot;Tukey&quot;, &quot;Venables&quot;, &quot;Tierney&quot;, &quot;Ripley&quot;, &quot;Ripley&quot;, &quot;McNeil&quot;, &quot;R Core&quot; )), title = c( &quot;Exploratory Data Analysis&quot;, &quot;Modern Applied Statistics ...&quot;, &quot;LISP-STAT&quot;, &quot;Spatial Statistics&quot;, &quot;Stochastic Simulation&quot;, &quot;Interactive Data Analysis&quot;, &quot;An Introduction to R&quot; ), other.author = c( NA, &quot;Ripley&quot;, NA, NA, NA, NA, &quot;Venables &amp; Smith&quot; ) ) authors #&gt; surname nationality deceased #&gt; 1 Tukey US yes #&gt; 2 Venables Australia no #&gt; 3 Tierney US no #&gt; 4 Ripley UK no #&gt; 5 McNeil Australia no authorN #&gt; nationality deceased name #&gt; 1 US yes Tukey #&gt; 2 Australia no Venables #&gt; 3 US no Tierney #&gt; 4 UK no Ripley #&gt; 5 Australia no McNeil books #&gt; name title other.author #&gt; 1 Tukey Exploratory Data Analysis &lt;NA&gt; #&gt; 2 Venables Modern Applied Statistics ... Ripley #&gt; 3 Tierney LISP-STAT &lt;NA&gt; #&gt; 4 Ripley Spatial Statistics &lt;NA&gt; #&gt; 5 Ripley Stochastic Simulation &lt;NA&gt; #&gt; 6 McNeil Interactive Data Analysis &lt;NA&gt; #&gt; 7 R Core An Introduction to R Venables &amp; Smith 默认找到同名的列，然后是同名的行合并，多余的没有匹配到的就丢掉 merge(authorN, books) #&gt; name nationality deceased title other.author #&gt; 1 McNeil Australia no Interactive Data Analysis &lt;NA&gt; #&gt; 2 Ripley UK no Spatial Statistics &lt;NA&gt; #&gt; 3 Ripley UK no Stochastic Simulation &lt;NA&gt; #&gt; 4 Tierney US no LISP-STAT &lt;NA&gt; #&gt; 5 Tukey US yes Exploratory Data Analysis &lt;NA&gt; #&gt; 6 Venables Australia no Modern Applied Statistics ... Ripley 还可以指定合并的列，先按照 surname 合并，留下 surname merge(authors, books, by.x = &quot;surname&quot;, by.y = &quot;name&quot;) #&gt; surname nationality deceased title other.author #&gt; 1 McNeil Australia no Interactive Data Analysis &lt;NA&gt; #&gt; 2 Ripley UK no Spatial Statistics &lt;NA&gt; #&gt; 3 Ripley UK no Stochastic Simulation &lt;NA&gt; #&gt; 4 Tierney US no LISP-STAT &lt;NA&gt; #&gt; 5 Tukey US yes Exploratory Data Analysis &lt;NA&gt; #&gt; 6 Venables Australia no Modern Applied Statistics ... Ripley 留下的是 name merge(books, authors, by.x = &quot;name&quot;, by.y = &quot;surname&quot;) #&gt; name title other.author nationality deceased #&gt; 1 McNeil Interactive Data Analysis &lt;NA&gt; Australia no #&gt; 2 Ripley Spatial Statistics &lt;NA&gt; UK no #&gt; 3 Ripley Stochastic Simulation &lt;NA&gt; UK no #&gt; 4 Tierney LISP-STAT &lt;NA&gt; US no #&gt; 5 Tukey Exploratory Data Analysis &lt;NA&gt; US yes #&gt; 6 Venables Modern Applied Statistics ... Ripley Australia no 为了比较清楚地观察几种合并的区别，这里提供对应的动画展示 https://github.com/gadenbuie/tidyexplain (inner, outer, left, right, cross) join 共5种合并方式详情请看 https://stackoverflow.com/questions/1299871 cbind 和 rbind 分别是按列和行合并数据框 2.8 数据去重 单个数值型向量去重，此时和 unique 函数作用一样 (x &lt;- c(9:20, 1:5, 3:7, 0:8)) #&gt; [1] 9 10 11 12 13 14 15 16 17 18 19 20 1 2 3 4 5 3 4 5 6 7 0 #&gt; [24] 1 2 3 4 5 6 7 8 ## extract unique elements x[!duplicated(x)] #&gt; [1] 9 10 11 12 13 14 15 16 17 18 19 20 1 2 3 4 5 6 7 0 8 unique(x) #&gt; [1] 9 10 11 12 13 14 15 16 17 18 19 20 1 2 3 4 5 6 7 0 8 数据框类型数据中，去除重复的行，这个重复可以是多个变量对应的向量 set.seed(123) df &lt;- data.frame( x = sample(0:1, 10, replace = T), y = sample(0:1, 10, replace = T), z = 1:10 ) df #&gt; x y z #&gt; 1 0 1 1 #&gt; 2 0 1 2 #&gt; 3 0 1 3 #&gt; 4 1 0 4 #&gt; 5 0 1 5 #&gt; 6 1 0 6 #&gt; 7 1 1 7 #&gt; 8 1 0 8 #&gt; 9 0 0 9 #&gt; 10 0 0 10 df[!duplicated(df[, 1:2]), ] #&gt; x y z #&gt; 1 0 1 1 #&gt; 4 1 0 4 #&gt; 7 1 1 7 #&gt; 9 0 0 9 2.9 数据聚合 分组求和 https://stackoverflow.com/questions/1660124 主要是分组统计 apropos(&quot;apply&quot;) #&gt; [1] &quot;apply&quot; &quot;dendrapply&quot; &quot;eapply&quot; &quot;kernapply&quot; &quot;lapply&quot; #&gt; [6] &quot;mapply&quot; &quot;rapply&quot; &quot;sapply&quot; &quot;tapply&quot; &quot;vapply&quot; # 分组求和 colSums colMeans max unique(iris$Species) #&gt; [1] setosa versicolor virginica #&gt; Levels: setosa versicolor virginica # 分类求和 # colSums(iris[iris$Species == &quot;setosa&quot;, -5]) # colSums(iris[iris$Species == &quot;virginica&quot;, -5]) colSums(iris[iris$Species == &quot;versicolor&quot;, -5]) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width #&gt; 296.8 138.5 213.0 66.3 # apply(iris[iris$Species == &quot;setosa&quot;, -5], 2, sum) # apply(iris[iris$Species == &quot;setosa&quot;, -5], 2, mean) # apply(iris[iris$Species == &quot;setosa&quot;, -5], 2, min) # apply(iris[iris$Species == &quot;setosa&quot;, -5], 2, max) apply(iris[iris$Species == &quot;setosa&quot;, -5], 2, quantile) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width #&gt; 0% 4.3 2.300 1.000 0.1 #&gt; 25% 4.8 3.200 1.400 0.2 #&gt; 50% 5.0 3.400 1.500 0.2 #&gt; 75% 5.2 3.675 1.575 0.3 #&gt; 100% 5.8 4.400 1.900 0.6 aggregate: Compute Summary Statistics of Data Subsets # 按分类变量 Species 分组求和 # aggregate(subset(iris, select = -Species), by = list(iris[, &quot;Species&quot;]), FUN = sum) aggregate(iris[, -5], list(iris[, 5]), sum) #&gt; Group.1 Sepal.Length Sepal.Width Petal.Length Petal.Width #&gt; 1 setosa 250.3 171.4 73.1 12.3 #&gt; 2 versicolor 296.8 138.5 213.0 66.3 #&gt; 3 virginica 329.4 148.7 277.6 101.3 # 先确定位置，假设有很多分类变量 ind &lt;- which(&quot;Species&quot; == colnames(iris)) # 分组统计 aggregate(iris[, -ind], list(iris[, ind]), sum) #&gt; Group.1 Sepal.Length Sepal.Width Petal.Length Petal.Width #&gt; 1 setosa 250.3 171.4 73.1 12.3 #&gt; 2 versicolor 296.8 138.5 213.0 66.3 #&gt; 3 virginica 329.4 148.7 277.6 101.3 按照 Species 划分的类别，分组计算，使用公式表示形式，右边一定是分类变量，否则会报错误或者警告，输出奇怪的结果，请读者尝试运行aggregate(Species ~ Sepal.Length, data = iris, mean)。公式法表示分组计算，~ 左手边可以做加 + 减 - 乘 * 除 / 取余 %% 等数学运算。下面以数据集 iris 为例，只对 Sepal.Length 按 Species 分组计算 aggregate(Sepal.Length ~ Species, data = iris, mean) #&gt; Species Sepal.Length #&gt; 1 setosa 5.006 #&gt; 2 versicolor 5.936 #&gt; 3 virginica 6.588 与上述分组统计结果一样的命令，在大数据集上， 与 aggregate 相比，tapply 要快很多，by 是 tapply 的包裹，处理速度差不多。读者可以构造伪随机数据集验证。 # tapply(iris$Sepal.Length, list(iris$Species), mean) with(iris, tapply(Sepal.Length, Species, mean)) #&gt; setosa versicolor virginica #&gt; 5.006 5.936 6.588 by(iris$Sepal.Length, iris$Species, mean) #&gt; iris$Species: setosa #&gt; [1] 5.006 #&gt; -------------------------------------------------------- #&gt; iris$Species: versicolor #&gt; [1] 5.936 #&gt; -------------------------------------------------------- #&gt; iris$Species: virginica #&gt; [1] 6.588 对所有变量按 Species 分组计算 aggregate(. ~ Species, data = iris, mean) #&gt; Species Sepal.Length Sepal.Width Petal.Length Petal.Width #&gt; 1 setosa 5.006 3.428 1.462 0.246 #&gt; 2 versicolor 5.936 2.770 4.260 1.326 #&gt; 3 virginica 6.588 2.974 5.552 2.026 对变量 Sepal.Length 和 Sepal.Width 求和后，按 Species 分组计算 aggregate(Sepal.Length + Sepal.Width ~ Species, data = iris, mean) #&gt; Species Sepal.Length + Sepal.Width #&gt; 1 setosa 8.434 #&gt; 2 versicolor 8.706 #&gt; 3 virginica 9.562 对多个分类变量做分组计算，在数据集 ChickWeight 中 Chick和Diet都是数字编码的分类变量，其中 Chick 是有序的因子变量，Diet 是无序的因子变量，而 Time 是数值型的变量，表示小鸡出生的天数。 # 查看数据 str(ChickWeight) #&gt; Classes &#39;nfnGroupedData&#39;, &#39;nfGroupedData&#39;, &#39;groupedData&#39; and &#39;data.frame&#39;: 578 obs. of 4 variables: #&gt; $ weight: num 42 51 59 64 76 93 106 125 149 171 ... #&gt; $ Time : num 0 2 4 6 8 10 12 14 16 18 ... #&gt; $ Chick : Ord.factor w/ 50 levels &quot;18&quot;&lt;&quot;16&quot;&lt;&quot;15&quot;&lt;..: 15 15 15 15 15 15 1.. #&gt; $ Diet : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 1 1 1 1 1 1 1 1 1 ... #&gt; - attr(*, &quot;formula&quot;)=Class &#39;formula&#39; language weight ~ Time | Chick #&gt; .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; #&gt; - attr(*, &quot;outer&quot;)=Class &#39;formula&#39; language ~Diet #&gt; .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; #&gt; - attr(*, &quot;labels&quot;)=List of 2 #&gt; ..$ x: chr &quot;Time&quot; #&gt; ..$ y: chr &quot;Body weight&quot; #&gt; - attr(*, &quot;units&quot;)=List of 2 #&gt; ..$ x: chr &quot;(days)&quot; #&gt; ..$ y: chr &quot;(gm)&quot; 查看数据集ChickWeight的前几行 head(ChickWeight) #&gt; weight Time Chick Diet #&gt; 1 42 0 1 1 #&gt; 2 51 2 1 1 #&gt; 3 59 4 1 1 #&gt; 4 64 6 1 1 #&gt; 5 76 8 1 1 .... str(ChickWeight) #&gt; Classes &#39;nfnGroupedData&#39;, &#39;nfGroupedData&#39;, &#39;groupedData&#39; and &#39;data.frame&#39;: 578 obs. of 4 variables: #&gt; $ weight: num 42 51 59 64 76 93 106 125 149 171 ... #&gt; $ Time : num 0 2 4 6 8 10 12 14 16 18 ... #&gt; $ Chick : Ord.factor w/ 50 levels &quot;18&quot;&lt;&quot;16&quot;&lt;&quot;15&quot;&lt;..: 15 15 15 15 15 15 1.. #&gt; $ Diet : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 1 1 1 1 1 1 1 1 1 ... #&gt; - attr(*, &quot;formula&quot;)=Class &#39;formula&#39; language weight ~ Time | Chick .... 对于数据集ChickWeight中的有序变量Chick，aggregate 会按照既定顺序返回分组计算的结果 aggregate(weight ~ Chick, data = ChickWeight, mean) #&gt; Chick weight #&gt; 1 18 37.00000 #&gt; 2 16 49.71429 #&gt; 3 15 60.12500 #&gt; 4 13 67.83333 #&gt; 5 9 81.16667 .... aggregate(weight ~ Diet, data = ChickWeight, mean) #&gt; Diet weight #&gt; 1 1 102.6455 #&gt; 2 2 122.6167 #&gt; 3 3 142.9500 #&gt; 4 4 135.2627 分类变量没有用数字编码，以 CO2 数据集为例，该数据集描述草植对二氧化碳的吸收情况，Plant 是具有12个水平的有序的因子变量，Type表示植物的源头分别是魁北克(Quebec)和密西西比(Mississippi)，Treatment表示冷却(chilled)和不冷却(nonchilled)两种处理方式，conc表示周围环境中二氧化碳的浓度，uptake表示植物吸收二氧化碳的速率。 # 查看数据集 head(CO2) #&gt; Plant Type Treatment conc uptake #&gt; 1 Qn1 Quebec nonchilled 95 16.0 #&gt; 2 Qn1 Quebec nonchilled 175 30.4 #&gt; 3 Qn1 Quebec nonchilled 250 34.8 #&gt; 4 Qn1 Quebec nonchilled 350 37.2 #&gt; 5 Qn1 Quebec nonchilled 500 35.3 #&gt; 6 Qn1 Quebec nonchilled 675 39.2 str(CO2) #&gt; Classes &#39;nfnGroupedData&#39;, &#39;nfGroupedData&#39;, &#39;groupedData&#39; and &#39;data.frame&#39;: 84 obs. of 5 variables: #&gt; $ Plant : Ord.factor w/ 12 levels &quot;Qn1&quot;&lt;&quot;Qn2&quot;&lt;&quot;Qn3&quot;&lt;..: 1 1 1 1 1 1 1.. #&gt; $ Type : Factor w/ 2 levels &quot;Quebec&quot;,&quot;Mississippi&quot;: 1 1 1 1 1 1 1 1 .. #&gt; $ Treatment: Factor w/ 2 levels &quot;nonchilled&quot;,&quot;chilled&quot;: 1 1 1 1 1 1 1 1 .. #&gt; $ conc : num 95 175 250 350 500 675 1000 95 175 250 ... #&gt; $ uptake : num 16 30.4 34.8 37.2 35.3 39.2 39.7 13.6 27.3 37.1 ... #&gt; - attr(*, &quot;formula&quot;)=Class &#39;formula&#39; language uptake ~ conc | Plant #&gt; .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; #&gt; - attr(*, &quot;outer&quot;)=Class &#39;formula&#39; language ~Treatment * Type #&gt; .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; #&gt; - attr(*, &quot;labels&quot;)=List of 2 #&gt; ..$ x: chr &quot;Ambient carbon dioxide concentration&quot; #&gt; ..$ y: chr &quot;CO2 uptake rate&quot; #&gt; - attr(*, &quot;units&quot;)=List of 2 #&gt; ..$ x: chr &quot;(uL/L)&quot; #&gt; ..$ y: chr &quot;(umol/m^2 s)&quot; 对单个变量分组统计 aggregate(uptake ~ Plant, data = CO2, mean) #&gt; Plant uptake #&gt; 1 Qn1 33.22857 #&gt; 2 Qn2 35.15714 #&gt; 3 Qn3 37.61429 #&gt; 4 Qc1 29.97143 #&gt; 5 Qc3 32.58571 #&gt; 6 Qc2 32.70000 #&gt; 7 Mn3 24.11429 #&gt; 8 Mn2 27.34286 #&gt; 9 Mn1 26.40000 #&gt; 10 Mc2 12.14286 #&gt; 11 Mc3 17.30000 #&gt; 12 Mc1 18.00000 aggregate(uptake ~ Type, data = CO2, mean) #&gt; Type uptake #&gt; 1 Quebec 33.54286 #&gt; 2 Mississippi 20.88333 aggregate(uptake ~ Treatment, data = CO2, mean) #&gt; Treatment uptake #&gt; 1 nonchilled 30.64286 #&gt; 2 chilled 23.78333 对多个变量分组统计，查看二氧化碳吸收速率uptake随类型Type和处理方式Treatment aggregate(uptake ~ Type + Treatment, data = CO2, mean) #&gt; Type Treatment uptake #&gt; 1 Quebec nonchilled 35.33333 #&gt; 2 Mississippi nonchilled 25.95238 #&gt; 3 Quebec chilled 31.75238 #&gt; 4 Mississippi chilled 15.81429 tapply(CO2$uptake, list(CO2$Type, CO2$Treatment), mean) #&gt; nonchilled chilled #&gt; Quebec 35.33333 31.75238 #&gt; Mississippi 25.95238 15.81429 by(CO2$uptake, list(CO2$Type, CO2$Treatment), mean) #&gt; : Quebec #&gt; : nonchilled #&gt; [1] 35.33333 #&gt; -------------------------------------------------------- #&gt; : Mississippi #&gt; : nonchilled #&gt; [1] 25.95238 #&gt; -------------------------------------------------------- #&gt; : Quebec #&gt; : chilled #&gt; [1] 31.75238 #&gt; -------------------------------------------------------- #&gt; : Mississippi #&gt; : chilled #&gt; [1] 15.81429 在这个例子中 tapply 和 by 的输出结果的表示形式不一样，aggregate 返回一个 data.frame 数据框，tapply 返回一个表格 table，by 返回特殊的数据类型 by。 Function by is an object-oriented wrapper for tapply applied to data frames. # 分组求和 # by(iris[, 1], INDICES = list(iris$Species), FUN = sum) # by(iris[, 2], INDICES = list(iris$Species), FUN = sum) by(iris[, 3], INDICES = list(iris$Species), FUN = sum) #&gt; : setosa #&gt; [1] 73.1 #&gt; -------------------------------------------------------- #&gt; : versicolor #&gt; [1] 213 #&gt; -------------------------------------------------------- #&gt; : virginica #&gt; [1] 277.6 by(iris[1:3], INDICES = list(iris$Species), FUN = sum) #&gt; : setosa #&gt; [1] 494.8 #&gt; -------------------------------------------------------- #&gt; : versicolor #&gt; [1] 648.3 #&gt; -------------------------------------------------------- #&gt; : virginica #&gt; [1] 755.7 by(iris[1:3], INDICES = list(iris$Species), FUN = summary) #&gt; : setosa #&gt; Sepal.Length Sepal.Width Petal.Length #&gt; Min. :4.300 Min. :2.300 Min. :1.000 #&gt; 1st Qu.:4.800 1st Qu.:3.200 1st Qu.:1.400 #&gt; Median :5.000 Median :3.400 Median :1.500 #&gt; Mean :5.006 Mean :3.428 Mean :1.462 #&gt; 3rd Qu.:5.200 3rd Qu.:3.675 3rd Qu.:1.575 #&gt; Max. :5.800 Max. :4.400 Max. :1.900 #&gt; -------------------------------------------------------- #&gt; : versicolor #&gt; Sepal.Length Sepal.Width Petal.Length #&gt; Min. :4.900 Min. :2.000 Min. :3.00 #&gt; 1st Qu.:5.600 1st Qu.:2.525 1st Qu.:4.00 #&gt; Median :5.900 Median :2.800 Median :4.35 #&gt; Mean :5.936 Mean :2.770 Mean :4.26 #&gt; 3rd Qu.:6.300 3rd Qu.:3.000 3rd Qu.:4.60 #&gt; Max. :7.000 Max. :3.400 Max. :5.10 #&gt; -------------------------------------------------------- #&gt; : virginica #&gt; Sepal.Length Sepal.Width Petal.Length #&gt; Min. :4.900 Min. :2.200 Min. :4.500 #&gt; 1st Qu.:6.225 1st Qu.:2.800 1st Qu.:5.100 #&gt; Median :6.500 Median :3.000 Median :5.550 #&gt; Mean :6.588 Mean :2.974 Mean :5.552 #&gt; 3rd Qu.:6.900 3rd Qu.:3.175 3rd Qu.:5.875 #&gt; Max. :7.900 Max. :3.800 Max. :6.900 by(iris, INDICES = list(iris$Species), FUN = summary) #&gt; : setosa #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width #&gt; Min. :4.300 Min. :2.300 Min. :1.000 Min. :0.100 #&gt; 1st Qu.:4.800 1st Qu.:3.200 1st Qu.:1.400 1st Qu.:0.200 #&gt; Median :5.000 Median :3.400 Median :1.500 Median :0.200 #&gt; Mean :5.006 Mean :3.428 Mean :1.462 Mean :0.246 #&gt; 3rd Qu.:5.200 3rd Qu.:3.675 3rd Qu.:1.575 3rd Qu.:0.300 #&gt; Max. :5.800 Max. :4.400 Max. :1.900 Max. :0.600 #&gt; Species #&gt; setosa :50 #&gt; versicolor: 0 #&gt; virginica : 0 #&gt; #&gt; #&gt; #&gt; -------------------------------------------------------- #&gt; : versicolor #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width #&gt; Min. :4.900 Min. :2.000 Min. :3.00 Min. :1.000 #&gt; 1st Qu.:5.600 1st Qu.:2.525 1st Qu.:4.00 1st Qu.:1.200 #&gt; Median :5.900 Median :2.800 Median :4.35 Median :1.300 #&gt; Mean :5.936 Mean :2.770 Mean :4.26 Mean :1.326 #&gt; 3rd Qu.:6.300 3rd Qu.:3.000 3rd Qu.:4.60 3rd Qu.:1.500 #&gt; Max. :7.000 Max. :3.400 Max. :5.10 Max. :1.800 #&gt; Species #&gt; setosa : 0 #&gt; versicolor:50 #&gt; virginica : 0 #&gt; #&gt; #&gt; #&gt; -------------------------------------------------------- #&gt; : virginica #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width #&gt; Min. :4.900 Min. :2.200 Min. :4.500 Min. :1.400 #&gt; 1st Qu.:6.225 1st Qu.:2.800 1st Qu.:5.100 1st Qu.:1.800 #&gt; Median :6.500 Median :3.000 Median :5.550 Median :2.000 #&gt; Mean :6.588 Mean :2.974 Mean :5.552 Mean :2.026 #&gt; 3rd Qu.:6.900 3rd Qu.:3.175 3rd Qu.:5.875 3rd Qu.:2.300 #&gt; Max. :7.900 Max. :3.800 Max. :6.900 Max. :2.500 #&gt; Species #&gt; setosa : 0 #&gt; versicolor: 0 #&gt; virginica :50 #&gt; #&gt; #&gt; Group Averages Over Level Combinations of Factors 分组平均 str(warpbreaks) #&gt; &#39;data.frame&#39;: 54 obs. of 3 variables: #&gt; $ breaks : num 26 30 54 25 70 52 51 26 67 18 ... #&gt; $ wool : Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ tension: Factor w/ 3 levels &quot;L&quot;,&quot;M&quot;,&quot;H&quot;: 1 1 1 1 1 1 1 1 1 2 ... head(warpbreaks) #&gt; breaks wool tension #&gt; 1 26 A L #&gt; 2 30 A L #&gt; 3 54 A L #&gt; 4 25 A L #&gt; 5 70 A L #&gt; 6 52 A L ave(warpbreaks$breaks, warpbreaks$wool) #&gt; [1] 31.03704 31.03704 31.03704 31.03704 31.03704 31.03704 31.03704 #&gt; [8] 31.03704 31.03704 31.03704 31.03704 31.03704 31.03704 31.03704 #&gt; [15] 31.03704 31.03704 31.03704 31.03704 31.03704 31.03704 31.03704 #&gt; [22] 31.03704 31.03704 31.03704 31.03704 31.03704 31.03704 25.25926 #&gt; [29] 25.25926 25.25926 25.25926 25.25926 25.25926 25.25926 25.25926 #&gt; [36] 25.25926 25.25926 25.25926 25.25926 25.25926 25.25926 25.25926 #&gt; [43] 25.25926 25.25926 25.25926 25.25926 25.25926 25.25926 25.25926 #&gt; [50] 25.25926 25.25926 25.25926 25.25926 25.25926 with(warpbreaks, ave(breaks, tension, FUN = function(x) mean(x, trim = 0.1))) #&gt; [1] 35.6875 35.6875 35.6875 35.6875 35.6875 35.6875 35.6875 35.6875 #&gt; [9] 35.6875 26.3125 26.3125 26.3125 26.3125 26.3125 26.3125 26.3125 #&gt; [17] 26.3125 26.3125 21.0625 21.0625 21.0625 21.0625 21.0625 21.0625 #&gt; [25] 21.0625 21.0625 21.0625 35.6875 35.6875 35.6875 35.6875 35.6875 #&gt; [33] 35.6875 35.6875 35.6875 35.6875 26.3125 26.3125 26.3125 26.3125 #&gt; [41] 26.3125 26.3125 26.3125 26.3125 26.3125 21.0625 21.0625 21.0625 #&gt; [49] 21.0625 21.0625 21.0625 21.0625 21.0625 21.0625 # 分组求和 with(warpbreaks, ave(breaks, tension, FUN = function(x) sum(x))) #&gt; [1] 655 655 655 655 655 655 655 655 655 475 475 475 475 475 475 475 475 #&gt; [18] 475 390 390 390 390 390 390 390 390 390 655 655 655 655 655 655 655 #&gt; [35] 655 655 475 475 475 475 475 475 475 475 475 390 390 390 390 390 390 #&gt; [52] 390 390 390 # 分组求和 with(iris, ave(Sepal.Length, Species, FUN = function(x) sum(x))) #&gt; [1] 250.3 250.3 250.3 250.3 250.3 250.3 250.3 250.3 250.3 250.3 250.3 #&gt; [12] 250.3 250.3 250.3 250.3 250.3 250.3 250.3 250.3 250.3 250.3 250.3 #&gt; [23] 250.3 250.3 250.3 250.3 250.3 250.3 250.3 250.3 250.3 250.3 250.3 #&gt; [34] 250.3 250.3 250.3 250.3 250.3 250.3 250.3 250.3 250.3 250.3 250.3 #&gt; [45] 250.3 250.3 250.3 250.3 250.3 250.3 296.8 296.8 296.8 296.8 296.8 #&gt; [56] 296.8 296.8 296.8 296.8 296.8 296.8 296.8 296.8 296.8 296.8 296.8 #&gt; [67] 296.8 296.8 296.8 296.8 296.8 296.8 296.8 296.8 296.8 296.8 296.8 #&gt; [78] 296.8 296.8 296.8 296.8 296.8 296.8 296.8 296.8 296.8 296.8 296.8 #&gt; [89] 296.8 296.8 296.8 296.8 296.8 296.8 296.8 296.8 296.8 296.8 296.8 #&gt; [100] 296.8 329.4 329.4 329.4 329.4 329.4 329.4 329.4 329.4 329.4 329.4 #&gt; [111] 329.4 329.4 329.4 329.4 329.4 329.4 329.4 329.4 329.4 329.4 329.4 #&gt; [122] 329.4 329.4 329.4 329.4 329.4 329.4 329.4 329.4 329.4 329.4 329.4 #&gt; [133] 329.4 329.4 329.4 329.4 329.4 329.4 329.4 329.4 329.4 329.4 329.4 #&gt; [144] 329.4 329.4 329.4 329.4 329.4 329.4 329.4 2.10 表格统计 介绍操作表格的 table, addmargins, prop.table, xtabs, margin.table, ftabe 等函数 table 多个分类变量分组计数统计 介绍 warpbreaks 和 airquality 纽约空气质量监测数据集 二维的数据框 UCBAdmissions 1973 年加州大学伯克利分校的院系录取数据集 3维的列联表 Titanic 4维的列联表数据 泰坦尼克号幸存者数据集 with(warpbreaks, table(wool, tension)) #&gt; tension #&gt; wool L M H #&gt; A 9 9 9 #&gt; B 9 9 9 以 iris 数据集为例，table 的第一个参数是自己制造的第二个分类变量，原始分类变量是 Species with(iris, table(Sepal.check = Sepal.Length &gt; 7, Species)) #&gt; Species #&gt; Sepal.check setosa versicolor virginica #&gt; FALSE 50 50 38 #&gt; TRUE 0 0 12 with(iris, table(Sepal.check = Sepal.Length &gt; mean(Sepal.Length), Species)) #&gt; Species #&gt; Sepal.check setosa versicolor virginica #&gt; FALSE 50 24 6 #&gt; TRUE 0 26 44 以 airquality 数据集为例，看看月份中臭氧含量比较高的几天 aiq.tab &lt;- with(airquality, table(Oz.high = Ozone &gt; 80, Month)) aiq.tab #&gt; Month #&gt; Oz.high 5 6 7 8 9 #&gt; FALSE 25 9 20 19 27 #&gt; TRUE 1 0 6 7 2 对表格按行和列求和，即求表格的边际，查看总体情况 addmargins(aiq.tab, 1:2) #&gt; Month #&gt; Oz.high 5 6 7 8 9 Sum #&gt; FALSE 25 9 20 19 27 100 #&gt; TRUE 1 0 6 7 2 16 #&gt; Sum 26 9 26 26 29 116 臭氧含量超 80 的天数在每个月的占比，addmargins 的第二个参数 1 表示对列求和 aiq.prop &lt;- prop.table(aiq.tab, 2) aiq.prop #&gt; Month #&gt; Oz.high 5 6 7 8 9 #&gt; FALSE 0.96153846 1.00000000 0.76923077 0.73076923 0.93103448 #&gt; TRUE 0.03846154 0.00000000 0.23076923 0.26923077 0.06896552 aiq.marprop &lt;- addmargins(aiq.prop, 1) aiq.marprop #&gt; Month #&gt; Oz.high 5 6 7 8 9 #&gt; FALSE 0.96153846 1.00000000 0.76923077 0.73076923 0.93103448 #&gt; TRUE 0.03846154 0.00000000 0.23076923 0.26923077 0.06896552 #&gt; Sum 1.00000000 1.00000000 1.00000000 1.00000000 1.00000000 转换成百分比，将小数四舍五入转化为百分数，保留两位小数点 round(100 * aiq.marprop, 2) #&gt; Month #&gt; Oz.high 5 6 7 8 9 #&gt; FALSE 96.15 100.00 76.92 73.08 93.10 #&gt; TRUE 3.85 0.00 23.08 26.92 6.90 #&gt; Sum 100.00 100.00 100.00 100.00 100.00 pairs(airquality, panel = panel.smooth, main = &quot;airquality data&quot;) 以 UCBAdmissions 数据集为例，使用 xtabs 函数把数据组织成列联表，先查看数据的内容 UCBAdmissions #&gt; , , Dept = A #&gt; #&gt; Gender #&gt; Admit Male Female #&gt; Admitted 512 89 #&gt; Rejected 313 19 .... UCBA2DF &lt;- as.data.frame(UCBAdmissions) UCBA2DF #&gt; Admit Gender Dept Freq #&gt; 1 Admitted Male A 512 #&gt; 2 Rejected Male A 313 #&gt; 3 Admitted Female A 89 #&gt; 4 Rejected Female A 19 #&gt; 5 Admitted Male B 353 .... 接着将 UCBA2DF 数据集转化为表格的形式 UCBA2DF.tab &lt;- xtabs(Freq ~ Gender + Admit + Dept, data = UCBA2DF) ftable(UCBA2DF.tab) #&gt; Dept A B C D E F #&gt; Gender Admit #&gt; Male Admitted 512 353 120 138 53 22 #&gt; Rejected 313 207 205 279 138 351 #&gt; Female Admitted 89 17 202 131 94 24 #&gt; Rejected 19 8 391 244 299 317 将录取性别和院系进行对比 prop.table(margin.table(UCBA2DF.tab, c(1, 3)), 1) #&gt; Dept #&gt; Gender A B C D E F #&gt; Male 0.30657748 0.20810108 0.12077295 0.15496098 0.07097733 0.13861018 #&gt; Female 0.05885559 0.01362398 0.32316076 0.20435967 0.21416894 0.18583106 男生倾向于申请院系 A 和 B，女生倾向于申请院系 C 到 F，院系 A 和 B 是最容易录取的。 2.11 索引访问 which 与引用 [ 性能比较，在区间 \\([0,1]\\) 上生成 10 万个服从均匀分布的随机数，随机抽取其中\\(\\frac{1}{4}\\)。 n &lt;- 100000 x &lt;- runif(n) i &lt;- logical(n) i[sample(n, n / 4)] &lt;- TRUE microbenchmark::microbenchmark(x[i], x[which(i)], times = 1000) 使用 subset 函数与 [ 比较 2.12 多维数组 多维数组的行列是怎么定义的 ?array 轴的概念，画个图表示数组 array(1:27, c(3, 3, 3)) #&gt; , , 1 #&gt; #&gt; [,1] [,2] [,3] #&gt; [1,] 1 4 7 #&gt; [2,] 2 5 8 #&gt; [3,] 3 6 9 #&gt; #&gt; , , 2 #&gt; #&gt; [,1] [,2] [,3] #&gt; [1,] 10 13 16 #&gt; [2,] 11 14 17 #&gt; [3,] 12 15 18 #&gt; #&gt; , , 3 #&gt; #&gt; [,1] [,2] [,3] #&gt; [1,] 19 22 25 #&gt; [2,] 20 23 26 #&gt; [3,] 21 24 27 垂直于Z轴的平面去截三维立方体，3 代表 z 轴，得到三个截面（二维矩阵） asplit(array(1:27, c(3, 3, 3)), 3) #&gt; [[1]] #&gt; [,1] [,2] [,3] #&gt; [1,] 1 4 7 #&gt; [2,] 2 5 8 #&gt; [3,] 3 6 9 #&gt; #&gt; [[2]] #&gt; [,1] [,2] [,3] #&gt; [1,] 10 13 16 #&gt; [2,] 11 14 17 #&gt; [3,] 12 15 18 #&gt; #&gt; [[3]] #&gt; [,1] [,2] [,3] #&gt; [1,] 19 22 25 #&gt; [2,] 20 23 26 #&gt; [3,] 21 24 27 对每个二维矩阵按列求和 lapply(asplit(array(1:27, c(3, 3, 3)), 3), apply, 2, sum) #&gt; [[1]] #&gt; [1] 6 15 24 #&gt; #&gt; [[2]] #&gt; [1] 33 42 51 #&gt; #&gt; [[3]] #&gt; [1] 60 69 78 asplit 和 lapply 组合处理多维数组的计算问题，多维数组17 三维数组的矩阵运算 abind 包提供更多的数组操作，如合并，替换 2.13 其它操作 成对的数据操作有 list 与 unlist、stack 与 unstack、class 与 unclass、attach 与 detach 以及 with 和 within，它们在数据操作过程中有时会起到一定的补充作用。 2.13.1 列表属性 # 创建列表 list(...) pairlist(...) # 转化列表 as.list(x, ...) ## S3 method for class &#39;environment&#39; as.list(x, all.names = FALSE, sorted = FALSE, ...) as.pairlist(x) # 检查列表 is.list(x) is.pairlist(x) alist(...) list 函数用来构造、转化和检查 R 列表对象。下面创建一个临时列表对象 tmp ，它包含两个元素 A 和 B，两个元素都是向量，前者是数值型，后者是字符型 (tmp &lt;- list(A = c(1, 2, 3), B = c(&quot;a&quot;, &quot;b&quot;))) #&gt; $A #&gt; [1] 1 2 3 #&gt; #&gt; $B #&gt; [1] &quot;a&quot; &quot;b&quot; unlist(x, recursive = TRUE, use.names = TRUE) unlist 函数将给定的列表对象 x 简化为原子向量 (atomic vector)，我们发现简化之后变成一个字符型向量 unlist(tmp) #&gt; A1 A2 A3 B1 B2 #&gt; &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;a&quot; &quot;b&quot; unlist(tmp, use.names = FALSE) #&gt; [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;a&quot; &quot;b&quot; unlist 的逆操作是 relist 2.13.2 堆叠向量 stack(x, ...) ## Default S3 method: stack(x, drop = FALSE, ...) ## S3 method for class &#39;data.frame&#39; stack(x, select, drop = FALSE, ...) unstack(x, ...) ## Default S3 method: unstack(x, form, ...) ## S3 method for class &#39;data.frame&#39; unstack(x, form, ...) stack 与 unstack 将多个向量堆在一起组成一个向量 # 查看数据集 PlantGrowth class(PlantGrowth) #&gt; [1] &quot;data.frame&quot; head(PlantGrowth) #&gt; weight group #&gt; 1 4.17 ctrl #&gt; 2 5.58 ctrl #&gt; 3 5.18 ctrl #&gt; 4 6.11 ctrl #&gt; 5 4.50 ctrl #&gt; 6 4.61 ctrl # 检查默认的公式 formula(PlantGrowth) #&gt; weight ~ group # 根据公式解除堆叠 # 下面等价于 unstack(PlantGrowth, form = weight ~ group) (pg &lt;- unstack(PlantGrowth)) #&gt; ctrl trt1 trt2 #&gt; 1 4.17 4.81 6.31 #&gt; 2 5.58 4.17 5.12 #&gt; 3 5.18 4.41 5.54 #&gt; 4 6.11 3.59 5.50 #&gt; 5 4.50 5.87 5.37 #&gt; 6 4.61 3.83 5.29 #&gt; 7 5.17 6.03 4.92 #&gt; 8 4.53 4.89 6.15 #&gt; 9 5.33 4.32 5.80 #&gt; 10 5.14 4.69 5.26 现在再将变量 pg 堆叠起来，还可以指定要堆叠的列 stack(pg) #&gt; values ind #&gt; 1 4.17 ctrl #&gt; 2 5.58 ctrl #&gt; 3 5.18 ctrl #&gt; 4 6.11 ctrl #&gt; 5 4.50 ctrl .... stack(pg, select = -ctrl) #&gt; values ind #&gt; 1 4.81 trt1 #&gt; 2 4.17 trt1 #&gt; 3 4.41 trt1 #&gt; 4 3.59 trt1 #&gt; 5 5.87 trt1 .... 形式上和 reshape 有一些相似之处，数据框可以由长变宽或由宽变长 2.13.3 属性转化 class(x) class(x) &lt;- value unclass(x) inherits(x, what, which = FALSE) oldClass(x) oldClass(x) &lt;- value class 和 unclass 函数用来查看、设置类属性和取消类属性，常用于面向对象的编程设计中 class(iris) #&gt; [1] &quot;data.frame&quot; class(iris$Species) #&gt; [1] &quot;factor&quot; unclass(iris$Species) #&gt; [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 #&gt; [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 #&gt; [71] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 #&gt; [106] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 #&gt; [141] 3 3 3 3 3 3 3 3 3 3 #&gt; attr(,&quot;levels&quot;) .... 2.13.4 绑定环境 attach(what, pos = 2L, name = deparse(substitute(what), backtick = FALSE), warn.conflicts = TRUE ) detach(name, pos = 2L, unload = FALSE, character.only = FALSE, force = FALSE ) attach 和 detach 是否绑定数据框的列名，不推荐操作，推荐使用 with attach(iris) head(Species) #&gt; [1] setosa setosa setosa setosa setosa setosa #&gt; Levels: setosa versicolor virginica detach(iris) 2.13.5 数据环境 with(data, expr, ...) within(data, expr, ...) ## S3 method for class &#39;list&#39; within(data, expr, keepAttrs = TRUE, ...) data 参数 data 用来构造表达式计算的环境。其默认值可以是一个环境，列表，数据框。在 within 函数中 data 参数只能是列表或数据框。 expr 参数 expr 包含要计算的表达式。在 within 中通常包含一个复合表达式，比如 { a &lt;- somefun() b &lt;- otherfun() ... rm(unused1, temp) } with 和 within 计算一组表达式，计算的环境是由数据构造的，后者可以修改原始的数据 with(mtcars, mpg[cyl == 8 &amp; disp &gt; 350]) #&gt; [1] 18.7 14.3 10.4 10.4 14.7 19.2 15.8 和下面计算的结果一样，但是更加简洁漂亮 mtcars$mpg[mtcars$cyl == 8 &amp; mtcars$disp &gt; 350] #&gt; [1] 18.7 14.3 10.4 10.4 14.7 19.2 15.8 within 函数可以修改原数据环境中的多个变量，比如删除、修改和添加等 # 原数据集 airquality head(airquality) #&gt; Ozone Solar.R Wind Temp Month Day #&gt; 1 41 190 7.4 67 5 1 #&gt; 2 36 118 8.0 72 5 2 #&gt; 3 12 149 12.6 74 5 3 #&gt; 4 18 313 11.5 62 5 4 #&gt; 5 NA NA 14.3 56 5 5 #&gt; 6 28 NA 14.9 66 5 6 aq &lt;- within(airquality, { lOzone &lt;- log(Ozone) # 取对数 Month &lt;- factor(month.abb[Month]) # 字符串型转因子型 cTemp &lt;- round((Temp - 32) * 5 / 9, 1) # 从华氏温度到摄氏温度转化 S.cT &lt;- Solar.R / cTemp # 使用新创建的变量 rm(Day, Temp) }) # 修改后的数据集 head(aq) #&gt; Ozone Solar.R Wind Month S.cT cTemp lOzone #&gt; 1 41 190 7.4 May 9.793814 19.4 3.713572 #&gt; 2 36 118 8.0 May 5.315315 22.2 3.583519 #&gt; 3 12 149 12.6 May 6.394850 23.3 2.484907 #&gt; 4 18 313 11.5 May 18.742515 16.7 2.890372 #&gt; 5 NA NA 14.3 May NA 13.3 NA #&gt; 6 28 NA 14.9 May NA 18.9 3.332205 下面再举一个复杂的绘图例子，这个例子来自 boxplot 函数 with(ToothGrowth, { boxplot(len ~ dose, boxwex = 0.25, at = 1:3 - 0.2, subset = (supp == &quot;VC&quot;), col = &quot;#4285f4&quot;, main = &quot;Guinea Pigs&#39; Tooth Growth&quot;, xlab = &quot;Vitamin C dose mg&quot;, ylab = &quot;tooth length&quot;, ylim = c(0, 35) ) boxplot(len ~ dose, add = TRUE, boxwex = 0.25, at = 1:3 + 0.2, subset = supp == &quot;OJ&quot;, col = &quot;#EA4335&quot; ) legend(2, 9, c(&quot;Ascorbic acid&quot;, &quot;Orange juice&quot;), fill = c(&quot;#4285f4&quot;, &quot;#EA4335&quot;) ) }) 将 boxplot 函数的 subset 参数单独提出来，调用数据子集选择函数 subset ，这里 with 中只包含一个表达式，所以也可以不用大括号 with( subset(ToothGrowth, supp == &quot;VC&quot;), boxplot(len ~ dose, boxwex = 0.25, at = 1:3 - 0.2, col = &quot;#4285f4&quot;, main = &quot;Guinea Pigs&#39; Tooth Growth&quot;, xlab = &quot;Vitamin C dose mg&quot;, ylab = &quot;tooth length&quot;, ylim = c(0, 35) ) ) with( subset(ToothGrowth, supp == &quot;OJ&quot;), boxplot(len ~ dose, add = TRUE, boxwex = 0.25, at = 1:3 + 0.2, col = &quot;#EA4335&quot; ) ) legend(2, 9, c(&quot;Ascorbic acid&quot;, &quot;Orange juice&quot;), fill = c(&quot;#4285f4&quot;, &quot;#EA4335&quot;) ) 2.14 运行环境 xfun::session_info() #&gt; R version 3.6.1 (2019-07-05) #&gt; Platform: x86_64-pc-linux-gnu (64-bit) #&gt; Running under: Debian GNU/Linux 10 (buster) #&gt; #&gt; Locale: #&gt; LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C #&gt; LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 #&gt; LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 #&gt; LC_PAPER=en_US.UTF-8 LC_NAME=C #&gt; LC_ADDRESS=C LC_TELEPHONE=C #&gt; LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C #&gt; #&gt; Package version: #&gt; base64enc_0.1.3 bookdown_0.12 codetools_0.2-16 compiler_3.6.1 #&gt; curl_4.0 digest_0.6.20 evaluate_0.14 glue_1.3.1 #&gt; graphics_3.6.1 grDevices_3.6.1 highr_0.8 htmltools_0.3.6 #&gt; jsonlite_1.6 knitr_1.23 magrittr_1.5 markdown_1.0 #&gt; methods_3.6.1 mime_0.7 Rcpp_1.0.2 rmarkdown_1.14 #&gt; stats_3.6.1 stringi_1.4.3 stringr_1.4.0 tinytex_0.14 #&gt; tools_3.6.1 utils_3.6.1 xfun_0.8 yaml_2.2.0 参考文献 "],
["dm-dplyr.html", "第 3 章 净土化操作 3.1 常用操作 3.2 高频问题 3.3 管道操作 3.4 运行环境", " 第 3 章 净土化操作 常用操作和高频问题需要合并进之前的 data-manipulation，本章只介绍向量化计算 以 dplyr 为核心的 tidyverse 风数据操作 管道风操作 在不同规模的数据集上，Base R，dplyr 和 data.table 的处理性能应该属于低、中、高档搭配的情形 更加高级的数据变形操作，特别是数据类型的一致性，方便后续的可视化和建模，引入 tidyverse，数据处理或者叫特征工程 Base R vs data.table vs dplyr 它们各有优点，所以都加以介绍 参考 Jozef Hajnala 博文。 关于 tidyverse 提供的数据操作不要移动到 Base R 对应的章节，这二者已经越行越远，本章主要讲并行或分布式数据操作工具，如 SparkR sparklyr 针对大数据集上的操 data.table 诞生于2006年4月15日（以在 CRAN 上发布的第一个版本时间为准），是基于 data.frame 的扩展和 Base R 的数据操作连贯一些，dplyr 诞生于2014年1月29日，号称数据操作的语法，其实二者套路一致，都是借用 SQL 语言的设计，实现方式不同罢了，前者主要依靠 C 语言完成底层数据操作，总代码量1.29M，C 占65.6%，后者主要依靠 C++ 语言完成底层数据操作，总代码量1.2M，C++ 占34.4%，上层的高级操作接口都是 R 语言。像这样的大神在写代码，码力应该差不多，编程语言会对数据操作的性能有比较大的影响，我想这也是为什么在很多场合下 data.table 霸榜！ 关于 data.table 和 dplyr 的对比，参看爆栈网的帖子 https://stackoverflow.com/questions/21435339 Base R 的数据操作的一致性问题参见统计之都帖子 https://d.cosx.org/d/420763 Malcolm Barrett 以幻灯片的形式呈现 dplyr 和 purrr 的基础用法 Charlotte Wickham 的课程 A introduction to purrr purrr-tutorial 关于引用 quotation 相比于 SQL， dplyr 在数据库操作的不足，这是一些比较难的部分 https://dbi.r-dbi.org/articles/dbi-1#sec:open-issues 函数式编程 Functional Programming Languages 用于数据处理 rpivotTable 动态数据透视表 fuzzyjoin Join tables together on inexact matching dtplyr dtplyr is the data.table backend for dplyr. It provides S3 methods for data.table objects so that dplyr works the way you expect. bplyr basic dplyr and tidyr functionality without the tidyverse dependencies SqlRender 基于 Java 语言，借助 rJava 包支持参数化的 SQL 语句，并且可以将一种 SQL 语句（如 Microsoft SQL Server）转化为多种SQL语句（如Oracle, PostgreSQL, Amazon RedShift, Impala, IBM Netezza, Google BigQuery, Microsoft PDW, and SQLite） fastmap 实现键值存储，提供新的数据结构 Roaring bitmaps Bitsets, also called bitmaps, are commonly used as fast data structures. library(tidyverse) 数据操作的语法 第一代 Base R 数据操作已在第 2 章详细介绍 第二代 reshape （退休）使用函数 melt 和 cast 重构(restructure)和聚合(aggregate)数据 reshape2 （退休）是 reshape 的继任者，功能和 reshape 类似，提供两个函数 melt 和 cast 聚合数据，因此不再介绍 reshape，而鉴于 reshape2 还在活跃使用中，故而以它为例介绍 melt 和 cast 函数 plyr （退休）统一拆分(split)，计算(apply)，合并(combine)的数据处理流，由 dplyr（用于data.frame） 和 purrr （用于 list）继任 第三代 dplyr 操作数据的语法及其扩展 sparklyr 给 dplyr 提供 Spark 接口支持 dbplyr 给 dplyr 提供 DBI 数据库接口支持 dtplyr 给 dplyr 提供 data.table 支持 tidyr 提供 spread 和 gather 两个函数清洗数据 Garrett Grolemund 在 RStudio 主要从事教育教学，参考 Materials for the Tidyverse Train-the-trainer workshop 和 The Tidyverse Cookbook Dirk Eddelbuettel 的 Getting Started in R – Tinyverse Edition 3.1 常用操作 dplyr 由 Hadley Wickham 主要由开发和维护，是Rstudio公司开源的用于数据处理的一大利器，该包号称“数据操作的语法”，与 ggplot2 对应，也就是说数据处理那一套已经建立完整的和SQL一样的功能。它们都遵循同样的处理逻辑，只不过一个用SQL写，一个用R语言写，处理效率差不多，R语言写的 SQL 会被翻译为 SQL 语句，再传至数据库查询，当然它也支持内存内的数据操作。目前 dplyr 以 dbplyr 为后端支持的数据库有：MySQL、PostgreSQL，SQLite等，完整的支持列表请看 这里，连接特定数据库，都是基于 DBI，DBI 即 Database Interface， 是使用C/C++开发的底层数据库接口，是一个统一的关系型数据库连接框架，需要根据不同的具体的数据库进行实例化，才可使用。 dplyr 常用的函数是6个： arrange 排序 filter 过滤 select 选择 mutate 变换 summarise 汇总 group_by 分组 以 GGplot2 自带的钻石数据集diamonds为例介绍 3.1.1 查看 除了直接打印数据集的前几行，tibble 包还提供 glimpse 函数查看数据集，而 Base R 默认查看方式是调用 str 函数 diamonds #&gt; # A tibble: 53,940 x 10 #&gt; carat cut color clarity depth table price x y z #&gt; &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 #&gt; 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 #&gt; 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 #&gt; 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 #&gt; 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 #&gt; 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 #&gt; # … with 5.393e+04 more rows glimpse(diamonds) #&gt; Observations: 53,940 #&gt; Variables: 10 #&gt; $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.… #&gt; $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Goo… #&gt; $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J,… #&gt; $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1,… #&gt; $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59… #&gt; $ table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, … #&gt; $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 3… #&gt; $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.… #&gt; $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.… #&gt; $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.… 表 3.1: dplyr 定义的数据对象类型 类型 含义 int 整型 integer dbl （单）双精度浮点类型 chr 字符（串）类型 dttm data-time 类型 lgl 布尔类型 fctr 因子类型 factor date 日期类型 表 3.1 中 dttm 和 date 类型代指 lubridate 包指定的日期对象 POSIXct、 POSIXlt、 Date、 chron、 yearmon、 yearqtr、 zoo、 zooreg、 timeDate、 xts、 its、 ti、 jul、 timeSeries 和 fts。 3.1.2 筛选 3.1.3 排序 3.1.4 聚合 3.1.5 合并 3.1.6 重塑 3.1.7 变换 3.1.8 去重 数据去重在 dplyr 中的实现18。 set.seed(123) df &lt;- data.frame( x = sample(0:1, 10, replace = T), y = sample(0:1, 10, replace = T), z = 1:10 ) df #&gt; x y z #&gt; 1 0 1 1 #&gt; 2 0 1 2 #&gt; 3 0 1 3 #&gt; 4 1 0 4 #&gt; 5 0 1 5 #&gt; 6 1 0 6 #&gt; 7 1 1 7 #&gt; 8 1 0 8 #&gt; 9 0 0 9 #&gt; 10 0 0 10 df %&gt;% group_by(x, y) %&gt;% filter(row_number(z) == 1) #&gt; # A tibble: 4 x 3 #&gt; # Groups: x, y [4] #&gt; x y z #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 0 1 1 #&gt; 2 1 0 4 #&gt; 3 1 1 7 #&gt; 4 0 0 9 3.2 高频问题 常用的数据操作包含 创建空的数据框或者说初始化一个数据框， 按指定的列对数据框排序， 选择特定的一些列，复杂情况是可能需要正则表达式从列名或者值中筛选 合并两个数据框，分为 (inner outer left right) 四种情况 3.2.1 初始化数据框 创建空的数据框，就是不包含任何行、记录[^create-an-empty-data-frame] empty_df &lt;- data.frame( Doubles = double(), Ints = integer(), Factors = factor(), Logicals = logical(), Characters = character(), stringsAsFactors = FALSE ) str(empty_df) #&gt; &#39;data.frame&#39;: 0 obs. of 5 variables: #&gt; $ Doubles : num #&gt; $ Ints : int #&gt; $ Factors : Factor w/ 0 levels: #&gt; $ Logicals : logi #&gt; $ Characters: chr 如果数据框 df 包含数据，现在要依据它创建一个空的数据框 empty_df = df[FALSE,] 还可以使用 structure 构造一个数据框，并且我们发现它的效率更高 s &lt;- function() structure(list( Date = as.Date(character()), File = character(), User = character() ), class = &quot;data.frame&quot; ) d &lt;- function() data.frame( Date = as.Date(character()), File = character(), User = character(), stringsAsFactors = FALSE ) # microbenchmark::microbenchmark(s(), d()) 3.2.2 移除缺失记录 只要行中包含缺失值，我们就把这样的行移除出去 airquality[complete.cases(airquality), ] #&gt; Ozone Solar.R Wind Temp Month Day #&gt; 1 41 190 7.4 67 5 1 #&gt; 2 36 118 8.0 72 5 2 #&gt; 3 12 149 12.6 74 5 3 #&gt; 4 18 313 11.5 62 5 4 #&gt; 7 23 299 8.6 65 5 7 #&gt; 8 19 99 13.8 59 5 8 #&gt; 9 8 19 20.1 61 5 9 #&gt; 12 16 256 9.7 69 5 12 #&gt; 13 11 290 9.2 66 5 13 .... 3.2.3 数据类型转化 str(PlantGrowth) #&gt; &#39;data.frame&#39;: 30 obs. of 2 variables: #&gt; $ weight: num 4.17 5.58 5.18 6.11 4.5 4.61 5.17 4.53 5.33 5.14 ... #&gt; $ group : Factor w/ 3 levels &quot;ctrl&quot;,&quot;trt1&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... bob &lt;- PlantGrowth i &lt;- sapply(bob, is.factor) bob[i] &lt;- lapply(bob[i], as.character) str(bob) #&gt; &#39;data.frame&#39;: 30 obs. of 2 variables: #&gt; $ weight: num 4.17 5.58 5.18 6.11 4.5 4.61 5.17 4.53 5.33 5.14 ... #&gt; $ group : chr &quot;ctrl&quot; &quot;ctrl&quot; &quot;ctrl&quot; &quot;ctrl&quot; ... 3.2.4 跨列分组求和 输入是一个数据框 data.frame，按照其中某一变量分组，然后计算任意数量的变量的行和和列和。 空气质量数据集 airquality 按月份 Month 分组，然后求取满足条件的列的和 Reduce(rbind, lapply(unique(airquality$Month), function(gv) { subdta &lt;- subset(airquality, subset = Month == gv) data.frame( Colsum = as.numeric( colSums(subdta[, grepl(&quot;[mM]&quot;, names(airquality))], na.rm = TRUE) ), Month = gv ) })) #&gt; Colsum Month #&gt; 1 2032 5 #&gt; 2 155 5 #&gt; 3 2373 6 #&gt; 4 180 6 #&gt; 5 2601 7 #&gt; 6 217 7 #&gt; 7 2603 8 #&gt; 8 248 8 #&gt; 9 2307 9 #&gt; 10 270 9 什么是函数式编程，R 语言环境下的函数式编程是如何操作的 3.3 管道操作 Stefan Milton Bache 开发了 magrittr 包实现管道操作，增加代码的可读性和维护性，但是这个 R 包的名字取的太奇葩，因为 记不住，它其实是一个复杂的法语发音，中式英语就叫它马格里特吧！这下应该好记多了吧！ 我要查看是否需要新添加一个 R 包依赖，假设该 R 包是 reticulate 没有出现在 DESCRIPTION 文件中，但是可能已经被其中某（个）些 R 包依赖了 &quot;reticulate&quot; %in% sort(unique(unlist(tools::package_dependencies(desc::desc_get_deps()$package, recursive = TRUE)))) #&gt; [1] FALSE 安装 pkg 的依赖 pkg &lt;- c( &quot;bookdown&quot;, &quot;e1071&quot;, &quot;formatR&quot;, &quot;lme4&quot;, &quot;mvtnorm&quot;, &quot;prettydoc&quot;, &quot;psych&quot;, &quot;reticulate&quot;, &quot;rstan&quot;, &quot;rstanarm&quot;, &quot;rticles&quot;, &quot;svglite&quot;, &quot;TMB&quot;, &quot;glmmTMB&quot; ) # 获取 pkg 的所有依赖 dep_pkg &lt;- tools::package_dependencies(pkg, recursive = TRUE) # 将列表 list 合并为向量 vector merge_pkg &lt;- Reduce(&quot;c&quot;, dep_pkg, accumulate = FALSE) # 所有未安装的 R 包 miss_pkg &lt;- setdiff(unique(merge_pkg), unique(.packages(TRUE))) # 除了 pkg 外，未安装的 R 包，安装 pkg 的依赖 sort(setdiff(miss_pkg, pkg)) #&gt; [1] &quot;foreign&quot; &quot;mnormt&quot; 转化为管道操作，增加可读性，[^pipe-r] 再举一个关于数据模拟的例子 模拟 0-1 序列， set.seed(2019) binom_sample &lt;- function(n) { sum(sample(x = c(0,1), size = n, prob = c(0.8, 0.2), replace = TRUE))/n } # 频率估计概率 one_prob &lt;- sapply(10^(seq(8)), binom_sample) # 估计的误差 one_abs &lt;- abs(one_prob - 0.2) one_abs #&gt; [1] 1.000e-01 1.000e-02 1.100e-02 4.400e-03 1.460e-03 3.980e-04 4.700e-06 #&gt; [8] 9.552e-05 似然估计 3.4 运行环境 xfun::session_info(&#39;tidyverse&#39;) #&gt; R version 3.6.1 (2019-07-05) #&gt; Platform: x86_64-pc-linux-gnu (64-bit) #&gt; Running under: Debian GNU/Linux 10 (buster) #&gt; #&gt; Locale: #&gt; LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C #&gt; LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 #&gt; LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 #&gt; LC_PAPER=en_US.UTF-8 LC_NAME=C #&gt; LC_ADDRESS=C LC_TELEPHONE=C #&gt; LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C #&gt; #&gt; Package version: #&gt; askpass_1.1 assertthat_0.2.1 backports_1.1.4 #&gt; base64enc_0.1.3 BH_1.69.0.1 broom_0.5.2 #&gt; callr_3.3.1 cellranger_1.1.0 cli_1.1.0 #&gt; clipr_0.7.0 colorspace_1.4.1 crayon_1.3.4 #&gt; curl_4.0 DBI_1.0.0 dbplyr_1.4.2 #&gt; digest_0.6.20 dplyr_0.8.3 ellipsis_0.2.0.1 #&gt; evaluate_0.14 fansi_0.4.0 forcats_0.4.0 #&gt; fs_1.3.1 generics_0.0.2 ggplot2_3.2.0 #&gt; glue_1.3.1 graphics_3.6.1 grDevices_3.6.1 #&gt; grid_3.6.1 gtable_0.3.0 haven_2.1.1 #&gt; highr_0.8 hms_0.5.0 htmltools_0.3.6 #&gt; httr_1.4.0 jsonlite_1.6 knitr_1.23 #&gt; labeling_0.3 lattice_0.20.38 lazyeval_0.2.2 #&gt; lubridate_1.7.4 magrittr_1.5 markdown_1.0 #&gt; MASS_7.3.51.4 Matrix_1.2.17 methods_3.6.1 #&gt; mgcv_1.8.28 mime_0.7 modelr_0.1.4 #&gt; munsell_0.5.0 nlme_3.1.140 openssl_1.4.1 #&gt; pillar_1.4.2 pkgconfig_2.0.2 plogr_0.2.0 #&gt; plyr_1.8.4 prettyunits_1.0.2 processx_3.4.1 #&gt; progress_1.2.2 ps_1.3.0 purrr_0.3.2 #&gt; R6_2.4.0 RColorBrewer_1.1.2 Rcpp_1.0.2 #&gt; readr_1.3.1 readxl_1.3.1 rematch_1.0.1 #&gt; reprex_0.3.0 reshape2_1.4.3 rlang_0.4.0 #&gt; rmarkdown_1.14 rstudioapi_0.10 rvest_0.3.4 #&gt; scales_1.0.0 selectr_0.4.1 splines_3.6.1 #&gt; stats_3.6.1 stringi_1.4.3 stringr_1.4.0 #&gt; sys_3.2 tibble_2.1.3 tidyr_0.8.3 #&gt; tidyselect_0.2.5 tidyverse_1.2.1 tinytex_0.14 #&gt; tools_3.6.1 utf8_1.1.4 utils_3.6.1 #&gt; vctrs_0.2.0 viridisLite_0.3.0 whisker_0.3.2 #&gt; withr_2.1.2 xfun_0.8 xml2_1.2.0 #&gt; yaml_2.2.0 zeallot_0.1.0 "],
["dv-spatio-temporal.html", "第 4 章 时空数据可视化 4.1 地图 4.2 空间数据对象", " 第 4 章 时空数据可视化 library(ggplot2) library(magrittr) options( ggplot2.continuous.colour = &quot;viridis&quot;, ggplot2.continuous.fill = &quot;viridis&quot; ) Robert J. Hijmans [^Robert-Hijmans] 开发了 raster 包用于网格空间数据的读、写、操作、分析和建模，同时维护了空间数据分析的网站 https://www.rspatial.org Edzer Pebesma [^Edzer-Pebesma] 和 Roger Bivand 等创建了 sp 包定义了空间数据类型和方法，提供了大量的空间数据操作方法，同时维护了空间数据对象 sp 的绘图网站 https://edzer.github.io/sp/，他们也一起合作写了新书 Spatial Data Science，提供了在线 网页版书籍及其 源代码 Edzer Pebesma 后来开发了 sf 包重新定义了空间数据对象和操作方法，并维护了空间数据分析、建模和可视化网站 https://www.r-spatial.org/ 课程案例学习 2018-Introduction to Geospatial Raster and Vector Data with R 空间数据分析课程 Peter Ellis 新西兰大选和普查数据 More cartograms of New Zealand census data: district and city level 2017-Mapping oil production by country in R 石油产量在全球的分布 2017-How to highlight countries on a map 高亮地图上的国家 2017-Mapping With Sf: Part 3 Data Visualization Shiny Apps 数据可视化核密度估计 In this app I identify crime hotspots using a bivariate density estimation strategy Association of Statisticians of American Religious Bodies (ASARB) viridis USA map 出租车行车轨迹数据 Geospatial processing with Clickhouse-CARTO Blog 4.1 地图 我们先来看看中国及其周边，这个地图的缺陷就是中国南海及九段线没有标记，台湾和中国大陆不是一种颜色标记，地图数据来自 R 包的 maps 和 mapdata library(maps) library(mapdata) east_asia &lt;- map_data(&quot;worldHires&quot;, region = c( &quot;Japan&quot;, &quot;Taiwan&quot;, &quot;China&quot;, &quot;North Korea&quot;, &quot;South Korea&quot; ) ) ggplot(east_asia, aes(x = long, y = lat, group = group, fill = region)) + geom_polygon(colour = &quot;black&quot;) + scale_fill_brewer(palette = &quot;Set2&quot;) 4.1.1 投影和观察方位 世界地图引发的 https://d.cosx.org/d/420808 worldmap &lt;- map_data(&quot;world&quot;) ggplot(worldmap, aes(long, lat, group = group)) + geom_polygon() + coord_map( xlim = c(-120, 40), ylim = c(30, 90), orientation = c(90, 0, 0) ) # 默认mercator投影下的默认视角 c(90, 0, mean(range(x))) ggplot(worldmap, aes(long, lat, group = group)) + geom_polygon(aes(fill = region), show.legend = FALSE) + coord_map( xlim = c(-120, 40), ylim = c(30, 90) ) # 相当于 (-120 + 40)/2 = -40 ggplot(worldmap, aes(long, lat, group = group)) + geom_polygon(aes(fill = region), show.legend = FALSE) + coord_map( xlim = c(-120, 40), ylim = c(30, 90), orientation = c(90, 0, -40) ) # 从西经 120 度到东经 40度 不同的看待 range(x) ggplot(worldmap, aes(long, lat, group = group)) + geom_polygon(aes(fill = region), show.legend = FALSE) + coord_map( xlim = c(-120, 40), ylim = c(30, 90), orientation = c(90, 0, -20) ) # 换观察角度 ggplot(worldmap, aes(long, lat, group = group)) + geom_polygon(aes(fill = region), show.legend = FALSE) + coord_map( xlim = c(-120, 40), ylim = c(30, 90), orientation = c(90, 0, 0) ) # 换投影坐标系 ggplot(worldmap, aes(long, lat, group = group)) + geom_polygon(aes(fill = region), show.legend = FALSE) + coord_map(&quot;ortho&quot;, xlim = c(-120, 40), ylim = c(30, 90) ) # 二者皆换 ggplot(worldmap, aes(long, lat, group = group)) + geom_polygon(aes(fill = region), show.legend = FALSE) + coord_map(&quot;ortho&quot;, xlim = c(-120, 40), ylim = c(30, 90), orientation = c(90, 0, 0) ) 4.1.2 美国各州犯罪率 美国各州的犯罪率数据 USArrests 准备数据集 crimes &lt;- data.frame(state = tolower(rownames(USArrests)), USArrests) crimesm &lt;- reshape2::melt(crimes, id = 1) head(crimesm) #&gt; state variable value #&gt; 1 alabama Murder 13.2 #&gt; 2 alaska Murder 10.0 #&gt; 3 arizona Murder 8.1 #&gt; 4 arkansas Murder 8.8 #&gt; 5 california Murder 9.0 #&gt; 6 colorado Murder 7.9 添加地图数据 library(maps) states_map &lt;- map_data(&quot;state&quot;) head(states_map) #&gt; long lat group order region subregion #&gt; 1 -87.46201 30.38968 1 1 alabama &lt;NA&gt; #&gt; 2 -87.48493 30.37249 1 2 alabama &lt;NA&gt; #&gt; 3 -87.52503 30.37249 1 3 alabama &lt;NA&gt; #&gt; 4 -87.53076 30.33239 1 4 alabama &lt;NA&gt; #&gt; 5 -87.57087 30.32665 1 5 alabama &lt;NA&gt; #&gt; 6 -87.58806 30.32665 1 6 alabama &lt;NA&gt; 绘图 ggplot(crimes, aes(map_id = state)) + geom_map(aes(fill = Murder), map = states_map) + expand_limits(x = states_map$long, y = states_map$lat) + coord_map() 图 4.1: 1973年美国各州的犯罪率数据 ggplot(crimesm, aes(map_id = state)) + geom_map(aes(fill = value), map = states_map) + expand_limits(x = states_map$long, y = states_map$lat) + facet_wrap(~variable) 图 4.2: 四类犯罪在各州的分布 4.1.3 斐济地震带 比较 viridis 和 Spectral 两块调色板，如图 4.3 所示，可见 Spectral 的可识别性高些 dat &lt;- as.data.frame(cbind(rep(1948 + seq(12), each = 12), rep(seq(12), 12), AirPassengers)) colnames(dat) &lt;- c(&quot;year&quot;, &quot;month&quot;, &quot;passengers&quot;) ggplot(data = dat, aes(as.factor(year), as.factor(month))) + geom_point(aes(colour = passengers), pch = 15, size = 8) + scale_colour_distiller(palette = &quot;Spectral&quot;) + labs(x = &quot;Year&quot;, y = &quot;Month&quot;) + theme_minimal() 图 4.3: viridis 和 Spectral对比 再举栗子，图4.4是正负例对比，其中好在哪里呢？这张图要表达美国黄石国家公园的老忠实泉间歇喷发的时间规律，那么好的标准就是层次分明，以突出不同颜色之间的时间差异。这个差异，还要看起来不那么费眼睛，越一目了然越好。 library(gridExtra) erupt &lt;- ggplot(faithfuld, aes(waiting, eruptions, fill = density)) + geom_raster() + scale_x_continuous(NULL, expand = c(0, 0)) + scale_y_continuous(NULL, expand = c(0, 0)) + theme(legend.position = &quot;none&quot;) erupt1 &lt;- erupt + scale_fill_gradientn(colours = gray.colors(7)) erupt2 &lt;- erupt + scale_fill_distiller(palette = &quot;Spectral&quot;) erupt3 &lt;- erupt + scale_fill_gradientn(colours = terrain.colors(7)) erupt4 &lt;- erupt grid.arrange(erupt1, erupt2, erupt3, erupt4, ncol = 2) 图 4.4: 美国黄石国家公园的老忠实泉 此处调用 RColorBrewer 中 Spectral 调色板，它本来只有11中颜色，通过 scale_colour_distiller 却可以把它映射到连续型数值变量 mag 上，发散型调色板本身的对比性也可以起到很好的区分度，如图 4.5 所示 FijiMap &lt;- map_data(&quot;worldHires&quot;, region = &quot;Fiji&quot;) ggplot(FijiMap, aes(x = long, y = lat)) + geom_map(map = FijiMap, aes(map_id = region), size = .2) + geom_point(data = quakes, aes(x = long, y = lat, colour = mag), pch = 16) + xlim(160, 195) + scale_colour_distiller(palette = &quot;Spectral&quot;) + scale_y_continuous(breaks = (-18:18) * 5) + coord_map(&quot;ortho&quot;, orientation = c(-10, 180, 0)) + labs(colour = &quot;Magnitude&quot;, x = &quot;Longitude&quot;, y = &quot;Latitude&quot;) + theme_minimal() 图 4.5: 斐济地震带 此外， colormap 包提供更加丰富的调色板，通过 scale_color_colormap 支持 ggplot2 绘图风格 4.1.4 美国各城镇失业率 # 数据来源 http://datasets.flowingdata.com/unemployment09.csv unemp &lt;- read.csv( file = &quot;http://datasets.flowingdata.com/unemployment09.csv&quot;, header = FALSE, stringsAsFactors = FALSE ) names(unemp) &lt;- c( &quot;id&quot;, &quot;state_fips&quot;, &quot;county_fips&quot;, &quot;name&quot;, &quot;year&quot;, &quot;?&quot;, &quot;?&quot;, &quot;?&quot;, &quot;rate&quot; ) unemp$county &lt;- tolower(gsub(&quot; County, [A-Z]{2}&quot;, &quot;&quot;, unemp$name)) unemp$state &lt;- gsub(&quot;^.*([A-Z]{2}).*$&quot;, &quot;\\\\1&quot;, unemp$name) county_df &lt;- map_data(&quot;county&quot;) names(county_df) &lt;- c(&quot;long&quot;, &quot;lat&quot;, &quot;group&quot;, &quot;order&quot;, &quot;state_name&quot;, &quot;county&quot;) county_df$state &lt;- state.abb[match(county_df$state_name, tolower(state.name))] county_df$state_name &lt;- NULL state_df &lt;- map_data(&quot;state&quot;) # Combine together choropleth &lt;- merge(county_df, unemp, by = c(&quot;state&quot;, &quot;county&quot;)) choropleth &lt;- choropleth[order(choropleth$order), ] choropleth$rate_d &lt;- cut(choropleth$rate, breaks = c(seq(0, 10, by = 2), 35)) library(ggthemes) ggplot(choropleth, aes(long, lat, group = group)) + geom_polygon(aes(fill = rate_d), colour = alpha(&quot;white&quot;, 1 / 4), size = 0.2) + geom_polygon(data = state_df, colour = &quot;white&quot;, fill = NA) + scale_fill_brewer(palette = &quot;PuRd&quot;) + labs( fill = &quot;ratio&quot;, title = &quot;ratio of unemployment by county, 2009&quot;, caption = &quot;data source: http://datasets.flowingdata.com/unemployment09.csv&quot; ) + coord_map(&quot;polyconic&quot;) + theme_map() 图 4.6: 2009年美国各城镇失业率 美国各地区失业率地图，配不同颜色， colormap 适合给静态图配色 4.2 空间数据对象 4.2.1 sp 空间数据对象，以类 sp 方式存储 (???) library(sp) crs = CRS(&quot;+init=epsg:28992&quot;) data(&quot;meuse&quot;) coordinates(meuse) &lt;- ~x+y proj4string(meuse) &lt;- crs class(meuse) #&gt; [1] &quot;SpatialPointsDataFrame&quot; #&gt; attr(,&quot;package&quot;) #&gt; [1] &quot;sp&quot; proj4string(meuse) #&gt; [1] &quot;+init=epsg:28992 +proj=sterea +lat_0=52.15616055555555 +lon_0=5.38763888888889 +k=0.9999079 +x_0=155000 +y_0=463000 +ellps=bessel +towgs84=565.2369,50.0087,465.658,-0.406857,0.350733,-1.87035,4.0812 +units=m +no_defs&quot; plot(meuse, axes = TRUE) 图 4.7: sp 对象 crs.longlat &lt;- CRS(&quot;+init=epsg:4326&quot;) meuse.longlat &lt;- spTransform(meuse, crs.longlat) plot(meuse.longlat, axes = TRUE) 图 4.8: sp 对象 library(maptools) #&gt; Checking rgeos availability: TRUE fname &lt;- system.file(&quot;shapes/sids.shp&quot;, package = &quot;maptools&quot;) p4s &lt;- CRS(&quot;+proj=longlat +datum=NAD27&quot;) nc &lt;- readShapePoly(fname, proj4string = p4s) #&gt; Warning: readShapePoly is deprecated; use rgdal::readOGR or sf::st_read plot(nc, axes = TRUE, col = grey(1 - nc$SID79 / 57)) # Trellis maps arrow &lt;- list(&quot;SpatialPolygonsRescale&quot;, layout.north.arrow(2), offset = c(-76, 34), scale = 0.5, which = 2 ) spplot(nc, c(&quot;SID74&quot;, &quot;SID79&quot;), as.table = TRUE, scales = list(draw = T), sp.layout = arrow ) maptools 提供的 readShapePoly 函数去读取 shp 文件的方式已经过时，推荐使用 rgdal::readOGR 或者 sf::st_read 方式读取 4.2.2 raster raster 包定义了获取和操作空间 raster 类型数据集的类和方法，rasterVis 补充加强了 raster 包在数据可视化和交互方面的功能。可视化是基于 lattice 的 raster 包的开发已经被作者 Robert J. Hijmans 迁移到 Github 上啦，官方文档 https://www.rspatial.org/ 星号 * 标记的是 S3 方法 methods(plot) #&gt; [1] plot,ANY,ANY-method #&gt; [2] plot,color,ANY-method #&gt; [3] plot,Spatial,missing-method #&gt; [4] plot,SpatialGrid,missing-method #&gt; [5] plot,SpatialGridDataFrame,missing-method #&gt; [6] plot,SpatialLines,missing-method #&gt; [7] plot,SpatialMultiPoints,missing-method #&gt; [8] plot,SpatialPixels,missing-method #&gt; [9] plot,SpatialPixelsDataFrame,missing-method #&gt; [10] plot,SpatialPoints,missing-method #&gt; [11] plot,SpatialPolygons,missing-method #&gt; [12] plot.acf* #&gt; [13] plot.data.frame* #&gt; [14] plot.decomposed.ts* #&gt; [15] plot.default #&gt; [16] plot.dendrogram* #&gt; [17] plot.density* #&gt; [18] plot.ecdf #&gt; [19] plot.factor* #&gt; [20] plot.formula* #&gt; [21] plot.function #&gt; [22] plot.ggplot* #&gt; [23] plot.gtable* #&gt; [24] plot.hcl_palettes* #&gt; [25] plot.hclust* #&gt; [26] plot.histogram* #&gt; [27] plot.HoltWinters* #&gt; [28] plot.isoreg* #&gt; [29] plot.lm* #&gt; [30] plot.medpolish* #&gt; [31] plot.mlm* #&gt; [32] plot.ppr* #&gt; [33] plot.prcomp* #&gt; [34] plot.princomp* #&gt; [35] plot.profile.nls* #&gt; [36] plot.R6* #&gt; [37] plot.raster* #&gt; [38] plot.shingle* #&gt; [39] plot.spec* #&gt; [40] plot.stepfun #&gt; [41] plot.stl* #&gt; [42] plot.table* #&gt; [43] plot.trellis* #&gt; [44] plot.ts #&gt; [45] plot.tskernel* #&gt; [46] plot.TukeyHSD* #&gt; see &#39;?methods&#39; for accessing help and source code 查看函数的定义 getAnywhere(plot.raster) #&gt; A single object matching &#39;plot.raster&#39; was found #&gt; It was found in the following places #&gt; registered S3 method for plot from namespace graphics #&gt; namespace:graphics #&gt; with value #&gt; #&gt; function (x, y, xlim = c(0, ncol(x)), ylim = c(0, nrow(x)), xaxs = &quot;i&quot;, #&gt; yaxs = &quot;i&quot;, asp = 1, add = FALSE, ...) #&gt; { #&gt; if (!add) { #&gt; plot.new() #&gt; plot.window(xlim = xlim, ylim = ylim, asp = asp, xaxs = xaxs, #&gt; yaxs = yaxs) #&gt; } #&gt; rasterImage(x, 0, 0, ncol(x), nrow(x), ...) #&gt; } #&gt; &lt;bytecode: 0x55dc377eb7e0&gt; #&gt; &lt;environment: namespace:graphics&gt; rasterImage 函数来绘制图像，如果想知道 rasterImage 的内容可以继续看 getAnywhere(rasterImage) getAnywhere(rasterImage) #&gt; A single object matching &#39;rasterImage&#39; was found #&gt; It was found in the following places #&gt; package:graphics #&gt; namespace:graphics #&gt; with value #&gt; #&gt; function (image, xleft, ybottom, xright, ytop, angle = 0, interpolate = TRUE, #&gt; ...) #&gt; { #&gt; .External.graphics(C_raster, if (inherits(image, &quot;nativeRaster&quot;)) image else as.raster(image), #&gt; as.double(xleft), as.double(ybottom), as.double(xright), #&gt; as.double(ytop), as.double(angle), as.logical(interpolate), #&gt; ...) #&gt; invisible() #&gt; } #&gt; &lt;bytecode: 0x55dc3b301d20&gt; #&gt; &lt;environment: namespace:graphics&gt; 通过查看函数的帮助 ?rasterImage ，我们需要重点关注一下 参数 image 传递的 raster 对象 plot(c(100, 250), c(300, 450), type = &quot;n&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;) image &lt;- as.raster(matrix(0:1, ncol = 5, nrow = 3)) rasterImage(image, 100, 300, 150, 350, interpolate = FALSE) rasterImage(image, 100, 400, 150, 450) rasterImage(image, 200, 300, 200 + xinch(.5), 300 + yinch(.3), interpolate = FALSE ) rasterImage(image, 200, 400, 250, 450, angle = 15, interpolate = FALSE) 图 4.9: raster 图像 library(raster) #&gt; #&gt; Attaching package: &#39;raster&#39; #&gt; The following object is masked from &#39;package:magrittr&#39;: #&gt; #&gt; extract meuse.test &lt;- raster(x = system.file(&quot;external/test.grd&quot;, package=&quot;raster&quot;)) class(meuse.test) #&gt; [1] &quot;RasterLayer&quot; #&gt; attr(,&quot;package&quot;) #&gt; [1] &quot;raster&quot; plot(meuse.test, legend = F) 图 4.10: raster 对象 Edzer Pebesma 开发了 stars 包 # https://resources.rstudio.com/rstudio-conf-2019/spatial-data-science-in-the-tidyverse library(abind) library(sf) #&gt; Linking to GEOS 3.7.1, GDAL 2.4.0, PROJ 5.2.0 library(stars) x &lt;- system.file(&quot;tif/L7_ETMs.tif&quot;, package = &quot;stars&quot;) %&gt;% read_stars() ggplot() + geom_stars(data = x) + coord_equal() + facet_wrap(~band) + theme_void() + scale_fill_viridis_c() + scale_x_discrete(expand = c(0, 0)) + scale_y_discrete(expand = c(0, 0)) 4.2.3 sf nc &lt;- system.file(&quot;gpkg/nc.gpkg&quot;, package = &quot;sf&quot;) %&gt;% read_sf() nc2 &lt;- nc %&gt;% dplyr::select(SID74, SID79) %&gt;% tidyr::gather(VAR, SID, -geom) ggplot() + geom_sf(data = nc2, aes(fill = SID)) + facet_wrap(~VAR, ncol = 1) "],
["99-references.html", "参考文献", " 参考文献 "]
]
